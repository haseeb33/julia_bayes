{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocumentSet Class using mutable struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase, CSV, DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentSet"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct DocumentSet\n",
    "    documents::Array\n",
    "    document_size::Int\n",
    "    vocab_count::Int\n",
    "    vocabulary::Dict{}\n",
    "    reverse_vocabulary::Array\n",
    "end\n",
    "\n",
    "function documentset_readData(path::String, documentset_obj::DocumentSet)\n",
    "    open(path) do file\n",
    "        for doc in eachline(file)\n",
    "            documentset_addDocument(doc, documentset_obj)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function documentset_readData(texts::Array, documentset_obj::DocumentSet)\n",
    "    for doc in texts\n",
    "        documentset_addDocument(doc, documentset_obj)\n",
    "    end\n",
    "end\n",
    "\n",
    "function documentset_addDocument(line::String, documentset_obj::DocumentSet)\n",
    "    if isempty(line)\n",
    "        return nothing\n",
    "    end\n",
    "    words = split(line)\n",
    "    codes = []\n",
    "    for i in words\n",
    "        if haskey(documentset_obj.vocabulary, i)\n",
    "            push!(codes, documentset_obj.vocabulary[i])\n",
    "        else \n",
    "            documentset_obj.vocab_count+=1\n",
    "            documentset_obj.vocabulary[i] = documentset_obj.vocab_count\n",
    "            push!(documentset_obj.reverse_vocabulary, i)\n",
    "            push!(codes, documentset_obj.vocab_count)\n",
    "        end\n",
    "    end\n",
    "    push!(documentset_obj.documents, codes)\n",
    "    documentset_obj.document_size+=1\n",
    "end\n",
    "\n",
    "function documentset_addDocument(words::Array, documentset_obj::DocumentSet)\n",
    "    if isempty(words)\n",
    "        return nothing\n",
    "    end\n",
    "    codes = []\n",
    "    for i in words\n",
    "        if haskey(documentset_obj.vocabulary, i)\n",
    "            push!(codes, documentset_obj.vocabulary[i])\n",
    "        else \n",
    "            documentset_obj.vocab_count+=1\n",
    "            documentset_obj.vocabulary[i] = documentset_obj.vocab_count\n",
    "            push!(documentset_obj.reverse_vocabulary, i)\n",
    "            push!(codes, documentset_obj.vocab_count)\n",
    "        end\n",
    "    end\n",
    "    push!(documentset_obj.documents, codes)\n",
    "    documentset_obj.document_size+=1\n",
    "end\n",
    "\n",
    "function documentset_transform(line::String, documentset_obj::DocumentSet)\n",
    "    words = split(line)\n",
    "    codes = []\n",
    "    for i in words\n",
    "        code = get(documentset_obj.vocabulary, i, -1)\n",
    "        if code != -1\n",
    "            push!(codes, code)\n",
    "        end  \n",
    "    end\n",
    "    return sort(codes)\n",
    "end\n",
    "\n",
    "function documentset_sampleDocuments(numDocs::Int, documentset_obj::DocumentSet)\n",
    "    subD = sample(documentset_obj.documents, numDocs, replace = false)\n",
    "    return subD  \n",
    "end\n",
    "\n",
    "function documentset_OnlinesampleDocuments(numDocs::Int, iter::Int, documentset_obj::DocumentSet)\n",
    "    subD = documentset_obj.documents[(iter-1)*numDocs+1: (iter)*numDocs]\n",
    "    return subD\n",
    "end\n",
    "\n",
    "function documentset_getTermFreq(documentset_obj::DocumentSet)\n",
    "    tf = [0 for i=1:documentset_obj.vocab_count]\n",
    "    for doc in documentset_obj.documents\n",
    "        for w in doc\n",
    "            tf[w]+=1 \n",
    "        end\n",
    "    end\n",
    "    return tf\n",
    "end\n",
    "\n",
    "DocumentSet() = DocumentSet([],0, 0, Dict(), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class call and using data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global corpus = DocumentSet()\n",
    "documentset_readData(\"dummy.txt\", corpus) #Or use readData(readlines(\"dummy.txt\"), corpus)\n",
    "V = size(corpus.reverse_vocabulary)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 5\n"
     ]
    }
   ],
   "source": [
    "println(\"Total docs: $(corpus.document_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"This\", \"is\", \"dummy\", \"line\", \"number\", \"1\", \"2\", \"3\", \"4th\", \"5th\", \"6th\"]\n",
      "[3, 3, 6, 6, 3, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "println(corpus.reverse_vocabulary)\n",
    "println(documentset_getTermFreq(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Any,1}:\n",
       " Any[1, 2, 3, 4, 5, 6]\n",
       " Any[1, 2, 3, 4, 5, 7]\n",
       " Any[1, 2, 3, 4, 5, 8]\n",
       " Any[9, 3, 4]\n",
       " Any[10, 3, 4]\n",
       " Any[11, 3, 4]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 11 entries:\n",
       "  \"number\" => 5\n",
       "  \"1\"      => 6\n",
       "  \"is\"     => 2\n",
       "  \"2\"      => 7\n",
       "  \"6th\"    => 11\n",
       "  \"dummy\"  => 3\n",
       "  \"5th\"    => 10\n",
       "  \"line\"   => 4\n",
       "  \"This\"   => 1\n",
       "  \"4th\"    => 9\n",
       "  \"3\"      => 8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       "  3\n",
       "  4\n",
       " 10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentset_transform(\"5th dummy line\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " Any[1, 2, 3, 4, 5, 8]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentset_OnlinesampleDocuments(1,3, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentset_addDocument(\"6th dummy line\", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SymmetricDirichlet Class(not extending Dirichlet) using struct\n",
    "Don't need to create this class, we can use Dirichlet Class for such kind of variables as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SymmetricDirichlet"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct SymmetricDirichlet\n",
    "    K::Int16\n",
    "    alpha::Float16\n",
    "    sumAlpha::Float16\n",
    "end\n",
    "\n",
    "SymmetricDirichlet(K, a) = SymmetricDirichlet(K, a, K*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SymmetricDirichlet(10, Float16(0.01), Float16(0.1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=100\n",
    "alpha = [0.01 for i in 1:M];\n",
    "wordPrior = SymmetricDirichlet(V,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Class using mutable struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dirichlet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global MINIMUM_PARAM = 10E-200\n",
    "\n",
    "mutable struct Dirichlet\n",
    "    K::Int64\n",
    "    alpha::Array{Float64}\n",
    "    sumAlpha::Float64\n",
    "end\n",
    "\n",
    "function dirichlet_optimizeParam(Ck, ndkMax, C_, ndMax, numIteration, dirichlet_obj::Dirichlet)\n",
    "    function digammaRecurrence(nMax, C, z)\n",
    "        if z==0.0\n",
    "            return 0.0\n",
    "        end\n",
    "        \n",
    "        R=0; S=0;\n",
    "        for n in 1:nMax\n",
    "            R+= 1.0 / (n-1+z)\n",
    "            S+= C[n]*R\n",
    "        end\n",
    "        return S\n",
    "    end\n",
    "            \n",
    "    for i in 1:numIteration\n",
    "        demon = digammaRecurrence(ndMax, C_, dirichlet_obj.sumAlpha)\n",
    "        \n",
    "        for k in 1:dirichlet_obj.K\n",
    "            numer = digammaRecurrence(ndkMax[k], Ck[k], dirichlet_obj.alpha[k])\n",
    "            dirichlet_obj.alpha[k] *= (numer/demon)\n",
    "            dirichlet_obj.alpha[k] = max(dirichlet_obj.alpha[k], MINIMUM_PARAM)\n",
    "        end\n",
    "        dirichlet_obj.sumAlpha = sum(dirichlet_obj.alpha)\n",
    "    end\n",
    "end\n",
    "\n",
    "function dirichlet_set(param, dirichlet_obj::Dirichlet)\n",
    "    for k in 1:dirichlet_obj.K\n",
    "        dirichlet_obj.alpha[k] = param[k] > MINIMUM_PARAM ? param[k] : MINIMUM_PARAM\n",
    "    end\n",
    "end\n",
    "\n",
    "Dirichlet(param::Array) = Dirichlet(size(param)[1], param, sum(param))\n",
    "Dirichlet(K::Int, a::Float64) = Dirichlet(K, [a for i in 1:K], K*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "topicPrior = Dirichlet(alpha);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Polya using mutable struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polya"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct Polya\n",
    "    K::Int64\n",
    "    dir::Dirichlet\n",
    "    n::Array{Int64}\n",
    "    N::Int64\n",
    "end\n",
    "\n",
    "function polya_p(x::Int, polya_obj::Polya)\n",
    "    return (polya_obj.n[x]+polya_obj.dir.alpha[x]) / (polya_obj.N+polya_obj.dir.sumAlpha)\n",
    "end\n",
    "\n",
    "function polya_p(X::Array, polya_obj::Polya)\n",
    "    p = 1.0\n",
    "    for x in X\n",
    "        p*= polya_p(x, polya_obj)\n",
    "        polya_observe(x, polya_obj)\n",
    "    end\n",
    "    for x in X\n",
    "        polya_forget(x, polya_obj)\n",
    "    end\n",
    "    return p\n",
    "end\n",
    "  \n",
    "function polya_observe(x::Int, polya_obj::Polya)\n",
    "    polya_obj.n[x]+=1\n",
    "    polya_obj.N+=1  \n",
    "end\n",
    "\n",
    "function polya_observe(X::Array, polya_obj::Polya)\n",
    "    for x in X\n",
    "        polya_observe(x, polya_obj)\n",
    "    end\n",
    "end\n",
    "\n",
    "function polya_forget(x::Int, polya_obj::Polya)\n",
    "    polya_obj.n[x] -= 1\n",
    "    polya_obj.N -= 1\n",
    "end\n",
    "\n",
    "function polya_forget(X::Array, polya_obj::Polya)\n",
    "    for x in X\n",
    "        polya_forget(x, polya_obj)\n",
    "    end\n",
    "end\n",
    "\n",
    "function polya_getCount(k::Int, polya_obj::Polya)\n",
    "    return polya_obj.n[k]\n",
    "end\n",
    "\n",
    "Polya(param::Dirichlet) = Polya(param.K, param, [0 for i in 1:param.K], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler_class's sample function is used in LDA class so defining only the function instead of complete class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sampler_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Sampler_sample(p)\n",
    "    u = rand()*sum(p)\n",
    "    for i in 1:size(p)[1]\n",
    "        if u<=p[i]\n",
    "            return i\n",
    "        end\n",
    "        u-=p[i]\n",
    "    end\n",
    "    return size(p)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polya Test Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polya(3, Dirichlet(3, [3.0, 2.0, 2.0], 7.0), [0, 0, 0], 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = [3.0, 2.0, 2.0]\n",
    "sut = Polya(Dirichlet(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001082251082251082"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1,3,3,1,1,2]\n",
    "polya_p(X, sut) # In java test is(closeTo(0.00108225108, 10E-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corpus1 = DocumentSet()\n",
    "documentset_readData(\"news.txt\", corpus1)\n",
    "sut = Polya(Dirichlet(12, 1.0))\n",
    "for doc in corpus1.documents\n",
    "    for x in doc\n",
    "        polya_observe(x, sut)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.09375\n",
      "0.0625\n"
     ]
    }
   ],
   "source": [
    "println(polya_p(1, sut)) # in java is(0.125)\n",
    "println(polya_p(5, sut)) # in java is(0.09375)\n",
    "println(polya_p(8, sut)) # in java is(0.0625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class LDA using mutable struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct LDA\n",
    "    numIteration::Int64\n",
    "    M::Int64\n",
    "    topicDir::Dirichlet\n",
    "    wordPolya::Array{Polya}\n",
    "    X::Array{Any,1}\n",
    "    topicPolya::Array{Polya}\n",
    "    Samples::Array{Any}\n",
    "end\n",
    "\n",
    "function lda_sample(docs, lda_obj::LDA)\n",
    "    lda_obj.X = docs\n",
    "    D = size(docs)[1]\n",
    "    \n",
    "    samples = [[] for i=1:D]\n",
    "    for d in 1:D\n",
    "        Nd = size(docs[d])[1]\n",
    "        samples[d] = [0 for i in 1:Nd]\n",
    "        push!(lda_obj.topicPolya, Polya(lda_obj.topicDir))\n",
    "        \n",
    "        temp = []\n",
    "        for i in 1:Nd\n",
    "            randomSample = rand(1:lda_obj.M)\n",
    "            push!(temp, randomSample)\n",
    "            lda_addSample(d, i, randomSample, lda_obj)\n",
    "        end\n",
    "        samples[d] = temp\n",
    "    end\n",
    "    lda_obj.Samples = samples\n",
    "    lda_gibbsSampling(docs, lda_obj.numIteration, lda_obj)\n",
    "end\n",
    "\n",
    "function lda_gibbsSampling(docs, numIteration, lda_obj::LDA)\n",
    "    D = size(docs)[1]\n",
    "    ndMax = Int(floor(maximum([size(doc)[1] for doc in docs])))\n",
    "    \n",
    "    C_ = [0 for i in 1:ndMax+1]\n",
    "    for d in 1:D\n",
    "        C_[(size(docs[d])[1])+1]+=1 \n",
    "    end\n",
    "    samples = lda_obj.Samples\n",
    "    \n",
    "    for iteration in 1:numIteration\n",
    "        for d in 1:D\n",
    "            Nd = size(docs[d])[1]\n",
    "            for i in 1:Nd\n",
    "                if samples[d][i] != 0 lda_removeSample(d, i, samples[d][i], lda_obj) end\n",
    "                samples[d][i] = Sampler_sample(lda_posterior(d, i, lda_obj))\n",
    "                lda_addSample(d, i, samples[d][i], lda_obj)\n",
    "            end\n",
    "        end\n",
    "     \n",
    "        ndkMax = [0 for i in 1:lda_obj.M]\n",
    "        Ck = [[0 for j in 1:ndMax+1] for i in 1:lda_obj.M]\n",
    "        for m in 1:lda_obj.M\n",
    "            for d in 1:D\n",
    "                ndk = lda_obj.topicPolya[d].n[m]\n",
    "                Ck[m][ndk+1]+=1  \n",
    "                ndkMax[m] = max(ndkMax[m], ndk)\n",
    "            end\n",
    "        end\n",
    "        dirichlet_optimizeParam(Ck, ndkMax, C_, ndMax+1, 20, lda_obj.topicDir)\n",
    "    end  \n",
    "    lda_obj.Samples = samples\n",
    "end\n",
    "      \n",
    "function lda_posterior(d::Int, i::Int, lda_obj::LDA)\n",
    "    v = lda_obj.X[d][i]\n",
    "    posterior = [0.0 for i in 1:lda_obj.M]\n",
    "    for m in 1:lda_obj.M\n",
    "        posterior[m] = polya_p(m, lda_obj.topicPolya[d]) * polya_p(v, lda_obj.wordPolya[m])\n",
    "    end\n",
    "    return posterior\n",
    "end\n",
    "\n",
    "function lda_addSample(d::Int, i::Int, m::Int, lda_obj::LDA)\n",
    "    v = lda_obj.X[d][i]\n",
    "    polya_observe(m, lda_obj.topicPolya[d])\n",
    "    polya_observe(v, lda_obj.wordPolya[m])\n",
    "end\n",
    "\n",
    "function lda_removeSample(d::Int, i::Int, m::Int, lda_obj::LDA)\n",
    "    v = lda_obj.X[d][i]\n",
    "    polya_forget(m, lda_obj.topicPolya[d])\n",
    "    polya_forget(v, lda_obj.wordPolya[m])\n",
    "end\n",
    "\n",
    "function lda_wordPredict(m::Int, v::Int, lda_obj::LDA)\n",
    "    return polya_p(v, lda_obj.wordPolya[m])\n",
    "end\n",
    "\n",
    "function lda_topicPredict(d::Int, m::Int, lda_obj::LDA)\n",
    "    return polya_p(m, lda_obj.topicPolya[d])\n",
    "end\n",
    "\n",
    "function lda_topicN(topic_n::Int, top_n_words::Int, corpus::DocumentSet, lda_obj::LDA)\n",
    "    topic_proportion = [];\n",
    "    for v in 1:size(corpus.reverse_vocabulary)[1]\n",
    "        prop = lda_wordPredict(topic_n, v, lda_obj)\n",
    "        push!(topic_proportion, prop)\n",
    "    end\n",
    "    topic_words_idx = sortperm(topic_proportion, rev=true)[1:top_n_words]\n",
    "    top_words = []\n",
    "    for i in topic_words_idx\n",
    "        wrd = corpus.reverse_vocabulary[i]\n",
    "        push!(top_words, wrd)\n",
    "    end\n",
    "    topic_proportion = topic_proportion[topic_words_idx]\n",
    "    return top_words, [round(i, digits=5) for i in topic_proportion]\n",
    "end\n",
    "\n",
    "function lda_remove_word(word::String, topic::Int, corpus::DocumentSet, lda_obj::LDA)\n",
    "    word = corpus.vocabulary[word]\n",
    "    lda_remove_word(word, topic, corpus, lda_obj)\n",
    "end\n",
    "function lda_remove_word(word::Int, topic::Int, corpus::DocumentSet, lda_obj::LDA)\n",
    "    for doc in enumerate(corpus.documents)\n",
    "        for w in enumerate(doc[2])\n",
    "            if w[2]==word\n",
    "                if lda_obj.Samples[doc[1]][w[1]] == topic\n",
    "                    lda_removeSample(doc[1], w[1], lda_obj.Samples[doc[1]][w[1]], lda_obj)\n",
    "                    lda_obj.Samples[doc[1]][w[1]] = 0\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    param = copy(lda_obj.wordPolya[topic].dir.alpha)\n",
    "    param[word] = 10E-8 # Assign very small prior epsilone, but this should be in discussion\n",
    "    lda_obj.wordPolya[topic].dir = Dirichlet(param) #Reconstruct the same lda variable\n",
    "end\n",
    "\n",
    "function lda_add_word(word::String, topic::Int, corpus::DocumentSet, lda_obj::LDA)\n",
    "    word = corpus.vocabulary[word]\n",
    "    lda_add_word(word, topic, corpus, lda_obj)\n",
    "end\n",
    "function lda_add_word(word::Int, topic::Int, corpus::DocumentSet, lda_obj::LDA)\n",
    "    for doc in enumerate(corpus.documents)\n",
    "        for w in enumerate(doc[2])\n",
    "            if w[2]==word\n",
    "                if lda_obj.Samples[doc[1]][w[1]]!=topic\n",
    "                    lda_removeSample(doc[1], w[1], lda_obj.Samples[doc[1]][w[1]], lda_obj)\n",
    "                    lda_obj.Samples[doc[1]][w[1]] = 0\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    param = copy(lda_obj.wordPolya[topic].dir.alpha)\n",
    "    difference = maximum(lda_obj.wordPolya[topic].n) - lda_obj.wordPolya[topic].n[word] #important discussion part\n",
    "    param[word] = lda_obj.wordPolya[topic].dir.alpha[word] + difference\n",
    "    lda_obj.wordPolya[topic].dir = Dirichlet(param)\n",
    "end\n",
    "\n",
    "LDA(topicPrior::Dirichlet, wordPrior::Dirichlet) = LDA(200, topicPrior.K, topicPrior, [Polya(wordPrior) for i in 1:topicPrior.K], [[]], [], [[]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First compelte test starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = DocumentSet()\n",
    "documentset_readData(\"news-en.txt\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPrior = Dirichlet(12, 0.01)\n",
    "M = 3\n",
    "alpha = [0.01 for i in 1:M];\n",
    "topicPrior = Dirichlet(alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(topicPrior, wordPrior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sample(corpus.documents, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Polya,1}:\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [3, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0], 7)\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1], 7)\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [0, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0], 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.wordPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dirichlet(12, [0.01, 0.01, 3.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 3.119999999999998)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_add_word(\"tank\", 3, corpus, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_gibbsSampling(corpus.documents, 20, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Polya,1}:\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [3, 3, 0, 1, 2, 2, 0, 0, 1, 0, 1, 0], 13)\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1], 5)\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 3.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 3.119999999999998), [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.wordPolya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove word refinement example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_remove_word(2, 2, corpus, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"Olympic\", \"medal\", \"Marathon\", \"runner\", \"Russian\"]\n",
      "[0.32843, 0.32843, 0.16503, 0.16503, 0.00163]\n",
      "----------------------\n",
      "Any[\"Moscow\", \"Self-Defense\", \"culture\", \"Russian\", \"tank\"]\n",
      "[0.58789, 0.19727, 0.19727, 0.00195, 0.00195]\n",
      "----------------------\n",
      "Any[\"tank\", \"unit\", \"Defense\", \"era\", \"Moscow\"]\n",
      "[0.32897, 0.32897, 0.1653, 0.1653, 0.00164]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for m in 1:3\n",
    "    words, proportions = lda_topicN(m, 5, corpus, lda)\n",
    "    println(words)\n",
    "    println(proportions)\n",
    "    println(\"----------------------\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " Any[2, 0, 1, 1]\n",
       " Any[2, 2, 2, 0]\n",
       " Any[1, 1, 1, 1]\n",
       " Any[3, 3, 2, 2]\n",
       " Any[2, 3, 0, 3]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Polya,1}:\n",
       " Polya(3, Dirichlet(3, [0.5145607604073209, 0.9261521782051171, 0.6127333399243915], 2.0534462785368293), [0, 3, 0], 3)\n",
       " Polya(3, Dirichlet(3, [0.5145607604073209, 0.9261521782051171, 0.6127333399243915], 2.0534462785368293), [1, 2, 1], 4)\n",
       " Polya(3, Dirichlet(3, [0.5145607604073209, 0.9261521782051171, 0.6127333399243915], 2.0534462785368293), [2, 1, 0], 3)\n",
       " Polya(3, Dirichlet(3, [0.5145607604073209, 0.9261521782051171, 0.6127333399243915], 2.0534462785368293), [2, 0, 2], 4)\n",
       " Polya(3, Dirichlet(3, [0.5145607604073209, 0.9261521782051171, 0.6127333399243915], 2.0534462785368293), [1, 3, 0], 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_gibbsSampling(corpus.documents, 20, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Polya,1}:\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [0, 3, 0, 0, 2, 2, 0, 1, 0, 0, 1, 1], 10)\n",
       " Polya(12, Dirichlet(12, [0.01, 1.0e-7, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.11000009999999999), [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], 2)\n",
       " Polya(12, Dirichlet(12, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 0.12), [3, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0], 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " lda.wordPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Polya,1}:\n",
       " Polya(3, Dirichlet(3, [0.7953199927256611, 0.8516254126048577, 1.2788663335256298], 2.9258117388561486), [1, 0, 3], 4)\n",
       " Polya(3, Dirichlet(3, [0.7953199927256611, 0.8516254126048577, 1.2788663335256298], 2.9258117388561486), [0, 2, 2], 4)\n",
       " Polya(3, Dirichlet(3, [0.7953199927256611, 0.8516254126048577, 1.2788663335256298], 2.9258117388561486), [2, 0, 2], 4)\n",
       " Polya(3, Dirichlet(3, [0.7953199927256611, 0.8516254126048577, 1.2788663335256298], 2.9258117388561486), [0, 4, 0], 4)\n",
       " Polya(3, Dirichlet(3, [0.7953199927256611, 0.8516254126048577, 1.2788663335256298], 2.9258117388561486), [0, 0, 4], 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicPolya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual topic modeling on NIPS papers dataset(used in fragmentation project as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = CSV.read(\"papers.csv\", DataFrame);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_txt = papers.paper_text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "specialchars = ['!', '”', '#', '$', '%', '&', '’', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '>', '=', '@', '?', '[', ']', '^', '_', '{', '}', '|', '~']\n",
    "open(\"stopwords.txt\") do file\n",
    "        for word in eachline(file)\n",
    "            push!(stopwords, word)\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remove_stop_words (generic function with 1 method)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function remove_stop_words(docs::Array, stopwords::Array)\n",
    "    new_docs = []\n",
    "    for line in docs\n",
    "        doc = split(line)\n",
    "        temp = []\n",
    "        for word in doc\n",
    "            if !(word in stopwords) \n",
    "                word = replace.(word, specialchars => \"\")\n",
    "                if length(word) > 2 && tryparse(Float64, word) == nothing\n",
    "                    push!(temp, lowercase(word))\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        push!(new_docs, temp)\n",
    "    end\n",
    "    return new_docs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paper_txt = remove_stop_words(papers_txt, stopwords);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentSet(Any[], 0, 0, Dict{Any,Any}(), Any[])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = DocumentSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentset_readData(new_paper_txt, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399426"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.vocab_count #Previous implementation value 478255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPrior = Dirichlet(corpus.vocab_count, 0.01)\n",
    "M = 30\n",
    "alpha = [0.01 for i in 1:M];\n",
    "topicPrior = Dirichlet(alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(topicPrior, wordPrior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = lda_sample(corpus.documents, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = [];\n",
    "for m in 1:1\n",
    "    for v in 1:size(corpus.reverse_vocabulary)[1]\n",
    "        val = lda_wordPredict(m, v, lda)\n",
    "        push!(val1, val)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399426,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       "  136\n",
       " 1988\n",
       "  172\n",
       " 1995\n",
       "   25\n",
       " 2077\n",
       " 8085\n",
       " 2995\n",
       " 1825\n",
       "  355"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = sortperm(val1, rev=true)[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem\n",
      "optimization\n",
      "solution\n",
      "regression\n",
      "the\n",
      "regularization\n",
      "convex\n",
      "sparse\n",
      "constraints\n",
      "min\n"
     ]
    }
   ],
   "source": [
    "for i in top_words\n",
    "    println(corpus.reverse_vocabulary[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : Any[\"problem\", \"optimization\", \"solution\", \"regression\", \"the\", \"regularization\", \"convex\", \"sparse\", \"constraints\", \"min\"]\n",
      "Topic 2 : Any[\"tree\", \"node\", \"nodes\", \"trees\", \"game\", \"the\", \"search\", \"strategy\", \"games\", \"cost\"]\n",
      "Topic 3 : Any[\"time\", \"state\", \"the\", \"dynamics\", \"process\", \"learning\", \"rate\", \"system\", \"one\", \"point\"]\n",
      "Topic 4 : Any[\"network\", \"networks\", \"the\", \"learning\", \"input\", \"training\", \"neural\", \"units\", \"output\", \"hidden\"]\n",
      "Topic 5 : Any[\"model\", \"latent\", \"data\", \"topic\", \"models\", \"the\", \"number\", \"process\", \"dirichlet\", \"topics\"]\n",
      "Topic 6 : Any[\"matrix\", \"algorithm\", \"matrices\", \"sparse\", \"the\", \"rank\", \"tensor\", \"data\", \"norm\", \"subspace\"]\n",
      "Topic 7 : Any[\"data\", \"the\", \"number\", \"time\", \"using\", \"performance\", \"method\", \"algorithm\", \"set\", \"accuracy\"]\n",
      "Topic 8 : Any[\"gradient\", \"algorithm\", \"optimization\", \"convergence\", \"algorithms\", \"stochastic\", \"methods\", \"method\", \"descent\", \"convex\"]\n",
      "Topic 9 : Any[\"graph\", \"set\", \"nodes\", \"graphs\", \"algorithm\", \"edges\", \"the\", \"function\", \"edge\", \"node\"]\n",
      "Topic 10 : Any[\"neurons\", \"the\", \"neuron\", \"neural\", \"model\", \"spike\", \"activity\", \"cells\", \"stimulus\", \"input\"]\n",
      "Topic 11 : Any[\"the\", \"linear\", \"function\", \"vector\", \"case\", \"error\", \"this\", \"one\", \"matrix\", \"given\"]\n",
      "Topic 12 : Any[\"the\", \"system\", \"memory\", \"network\", \"neural\", \"input\", \"figure\", \"output\", \"circuit\", \"analog\"]\n",
      "Topic 13 : Any[\"the\", \"space\", \"local\", \"data\", \"basis\", \"figure\", \"linear\", \"natural\", \"using\", \"manifold\"]\n",
      "Topic 14 : Any[\"word\", \"words\", \"language\", \"the\", \"model\", \"models\", \"set\", \"text\", \"example\", \"one\"]\n",
      "Topic 15 : Any[\"image\", \"images\", \"object\", \"the\", \"features\", \"objects\", \"model\", \"recognition\", \"feature\", \"visual\"]\n",
      "Topic 16 : Any[\"learning\", \"control\", \"target\", \"the\", \"task\", \"model\", \"tasks\", \"system\", \"trajectory\", \"figure\"]\n",
      "Topic 17 : Any[\"model\", \"ranking\", \"the\", \"user\", \"data\", \"information\", \"items\", \"query\", \"users\", \"item\"]\n",
      "Topic 18 : Any[\"the\", \"model\", \"speech\", \"time\", \"state\", \"sequence\", \"data\", \"models\", \"using\", \"sequences\"]\n",
      "Topic 19 : Any[\"theorem\", \"probability\", \"distribution\", \"let\", \"sample\", \"the\", \"bound\", \"random\", \"log\", \"estimator\"]\n",
      "Topic 20 : Any[\"algorithm\", \"regret\", \"the\", \"online\", \"bound\", \"learning\", \"algorithms\", \"problem\", \"setting\", \"set\"]\n",
      "Topic 21 : Any[\"kernel\", \"kernels\", \"data\", \"test\", \"the\", \"space\", \"causal\", \"using\", \"methods\", \"feature\"]\n",
      "Topic 22 : Any[\"function\", \"loss\", \"functions\", \"learning\", \"the\", \"risk\", \"convex\", \"bound\", \"set\", \"error\"]\n",
      "Topic 23 : Any[\"neural\", \"deep\", \"networks\", \"network\", \"learning\", \"training\", \"layer\", \"model\", \"layers\", \"convolutional\"]\n",
      "Topic 24 : Any[\"model\", \"the\", \"human\", \"figure\", \"two\", \"subjects\", \"brain\", \"models\", \"data\", \"one\"]\n",
      "Topic 25 : Any[\"clustering\", \"data\", \"points\", \"distance\", \"cluster\", \"algorithm\", \"the\", \"clusters\", \"metric\", \"point\"]\n",
      "Topic 26 : Any[\"model\", \"distribution\", \"data\", \"gaussian\", \"models\", \"the\", \"bayesian\", \"posterior\", \"prior\", \"likelihood\"]\n",
      "Topic 27 : Any[\"policy\", \"state\", \"learning\", \"action\", \"the\", \"reward\", \"function\", \"value\", \"actions\", \"reinforcement\"]\n",
      "Topic 28 : Any[\"learning\", \"training\", \"classification\", \"data\", \"features\", \"feature\", \"set\", \"classifier\", \"label\", \"class\"]\n",
      "Topic 29 : Any[\"price\", \"revenue\", \"prices\", \"patients\", \"rst\", \"auctions\", \"patient\", \"risk\", \"buyer\", \"dened\"]\n",
      "Topic 30 : Any[\"variables\", \"inference\", \"models\", \"the\", \"algorithm\", \"graphical\", \"markov\", \"variable\", \"model\", \"belief\"]\n"
     ]
    }
   ],
   "source": [
    "for m in 1:M\n",
    "    val1 = []\n",
    "    for v in 1:size(corpus.reverse_vocabulary)[1]\n",
    "        val = lda_wordPredict(m, v, lda)\n",
    "        push!(val1, val)\n",
    "    end\n",
    "    top_words = sortperm(val1, rev=true)[1:10]\n",
    "    wrds = []\n",
    "    for i in top_words\n",
    "        push!(wrds, corpus.reverse_vocabulary[i])\n",
    "    end\n",
    "    println(\"Topic $m : $wrds\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
