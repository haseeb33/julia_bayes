{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Documents/Thesis/Git_Main/julia_bayes/TopicModels/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "using StatsBase, CSV, DataFrames\n",
    "Pkg.activate(\"TopicModels\")\n",
    "import TopicModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Run on Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = TopicModels.documentset_readData(\"news-en.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPrior = TopicModels.Dirichlet(12, 0.01)\n",
    "M = 3\n",
    "alpha = [0.01 for i in 1:M];\n",
    "topicPrior = TopicModels.Dirichlet(alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = TopicModels.LDA(topicPrior, wordPrior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = TopicModels.lda_sample(corpus.documents, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st topic and top 5 words along with vocab proportion\n",
    "words, proportions = TopicModels.lda_topicN(3, 4, corpus, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"medal\", \"runner\", \"era\", \"culture\"]\n",
      "[0.39258, 0.19727, 0.19727, 0.19727]\n"
     ]
    }
   ],
   "source": [
    "println(words)\n",
    "println(proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA on NIPS papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing papers and few preprocessing steps\n",
    "papers = CSV.read(\"papers.csv\", DataFrame);\n",
    "papers_txt = papers.paper_text;\n",
    "stopwords = []\n",
    "specialchars = ['!', '”', '#', '$', '%', '&', '’', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '>', '=', '@', '?', '[', ']', '^', '_', '{', '}', '|', '~']\n",
    "open(\"stopwords.txt\") do file\n",
    "    for word in eachline(file)\n",
    "        push!(stopwords, word)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clean_words(docs::Array, stopwords::Array, specialchars::Array) \n",
    "    #Remove stop words, special characters, numbers, and 2_or_less char words\n",
    "    new_docs = []\n",
    "    for line in docs\n",
    "        doc = split(line)\n",
    "        temp = []\n",
    "        for word in doc\n",
    "            word = lowercase(replace.(word, specialchars => \"\"))\n",
    "            if !(word in stopwords) \n",
    "                if length(word) > 2 && tryparse(Float64, word) == nothing\n",
    "                    push!(temp, word)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        push!(new_docs, temp)\n",
    "    end\n",
    "    return new_docs\n",
    "end\n",
    "\n",
    "papers_txt = clean_words(papers_txt, stopwords, specialchars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_txt = papers_txt[6242:7241]\n",
    "size(papers_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108086"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = TopicModels.documentset_readData(papers_txt);\n",
    "corpus.vocab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPrior = TopicModels.Dirichlet(corpus.vocab_count, 0.01)\n",
    "M = 20 # Number of topics\n",
    "alpha = [0.01 for i in 1:M];\n",
    "topicPrior = TopicModels.Dirichlet(alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = TopicModels.LDA(topicPrior, wordPrior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.lda_sample(corpus.documents, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"matrix\", \"sparse\", \"matrices\", \"time\", \"rank\", \"error\", \"problem\", \"linear\", \"algorithm\", \"tensor\"]\n",
      "Any[\"adversarial\", \"training\", \"generative\", \"data\", \"gan\", \"samples\", \"distribution\", \"generator\", \"discriminator\", \"objective\"]\n",
      "Any[\"policy\", \"learning\", \"state\", \"reinforcement\", \"reward\", \"action\", \"agent\", \"policies\", \"value\", \"using\"]\n",
      "Any[\"model\", \"models\", \"data\", \"used\", \"using\", \"speech\", \"figure\", \"prediction\", \"modeling\", \"neural\"]\n",
      "Any[\"distribution\", \"inference\", \"model\", \"data\", \"models\", \"log\", \"gaussian\", \"posterior\", \"variational\", \"bayesian\"]\n",
      "Any[\"time\", \"system\", \"control\", \"noise\", \"figure\", \"model\", \"state\", \"process\", \"memory\", \"point\"]\n",
      "Any[\"graph\", \"algorithm\", \"clustering\", \"nodes\", \"node\", \"set\", \"algorithms\", \"cluster\", \"graphs\", \"problem\"]\n",
      "Any[\"learning\", \"algorithm\", \"algorithms\", \"loss\", \"time\", \"online\", \"machine\", \"problem\", \"cost\", \"pages\"]\n",
      "Any[\"set\", \"algorithm\", \"local\", \"one\", \"data\", \"used\", \"new\", \"two\", \"structure\", \"figure\"]\n",
      "Any[\"learning\", \"deep\", \"image\", \"training\", \"images\", \"networks\", \"neural\", \"loss\", \"dataset\", \"classification\"]\n",
      "Any[\"data\", \"rules\", \"vector\", \"number\", \"using\", \"rule\", \"set\", \"two\", \"information\", \"similarity\"]\n",
      "Any[\"neurons\", \"activity\", \"neural\", \"model\", \"cells\", \"cell\", \"figure\", \"neuron\", \"visual\", \"brain\"]\n",
      "Any[\"network\", \"networks\", \"neural\", \"training\", \"learning\", \"error\", \"layer\", \"output\", \"input\", \"units\"]\n",
      "Any[\"gradient\", \"optimization\", \"convergence\", \"convex\", \"algorithm\", \"stochastic\", \"descent\", \"methods\", \"method\", \"rate\"]\n",
      "Any[\"bound\", \"regret\", \"bounds\", \"algorithm\", \"theorem\", \"probability\", \"log\", \"distribution\", \"complexity\", \"lemma\"]\n",
      "Any[\"function\", \"functions\", \"set\", \"case\", \"one\", \"problem\", \"given\", \"optimal\", \"let\", \"consider\"]\n",
      "Any[\"kernel\", \"data\", \"features\", \"feature\", \"learning\", \"methods\", \"space\", \"points\", \"distance\", \"using\"]\n",
      "Any[\"games\", \"game\", \"player\", \"players\", \"communication\", \"revenue\", \"agents\", \"strategy\", \"price\", \"equilibrium\"]\n",
      "Any[\"object\", \"attention\", \"image\", \"network\", \"objects\", \"task\", \"neural\", \"visual\", \"convolutional\", \"features\"]\n",
      "Any[\"data\", \"model\", \"regression\", \"risk\", \"causal\", \"classification\", \"classifier\", \"set\", \"selection\", \"using\"]\n"
     ]
    }
   ],
   "source": [
    "top_N = 10\n",
    "for i in 1:M\n",
    "    #println(\"Topic $i top $top_N words:\")\n",
    "    words, proportions = TopicModels.lda_topicN(i, top_N, corpus, lda);\n",
    "    println(words)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopicModels.Dirichlet(108086, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 1080.8500001000002)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicModels.lda_removeWord(\"model\", 20, corpus, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.lda_gibbsSampling(corpus.documents, 20, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"matrix\", \"sparse\", \"matrices\", \"rank\", \"algorithm\", \"time\", \"estimation\", \"norm\", \"tensor\", \"problem\"]\n",
      "Any[\"adversarial\", \"training\", \"generative\", \"data\", \"gan\", \"samples\", \"distribution\", \"generator\", \"discriminator\", \"objective\"]\n",
      "Any[\"policy\", \"learning\", \"state\", \"reward\", \"reinforcement\", \"action\", \"agent\", \"policies\", \"using\", \"value\"]\n",
      "Any[\"model\", \"models\", \"data\", \"using\", \"speech\", \"neural\", \"modeling\", \"figure\", \"network\", \"used\"]\n",
      "Any[\"model\", \"distribution\", \"inference\", \"data\", \"models\", \"log\", \"gaussian\", \"posterior\", \"variational\", \"bayesian\"]\n",
      "Any[\"time\", \"model\", \"system\", \"noise\", \"state\", \"control\", \"figure\", \"process\", \"memory\", \"systems\"]\n",
      "Any[\"graph\", \"algorithm\", \"clustering\", \"nodes\", \"node\", \"set\", \"algorithms\", \"problem\", \"cluster\", \"graphs\"]\n",
      "Any[\"learning\", \"algorithm\", \"algorithms\", \"loss\", \"time\", \"online\", \"problem\", \"machine\", \"cost\", \"pages\"]\n",
      "Any[\"set\", \"algorithm\", \"local\", \"two\", \"data\", \"figure\", \"one\", \"used\", \"new\", \"node\"]\n",
      "Any[\"learning\", \"deep\", \"training\", \"image\", \"images\", \"networks\", \"neural\", \"network\", \"loss\", \"classification\"]\n",
      "Any[\"data\", \"rules\", \"number\", \"rule\", \"vector\", \"using\", \"probability\", \"similarity\", \"two\", \"quantization\"]\n",
      "Any[\"neurons\", \"activity\", \"neural\", \"cells\", \"neuron\", \"visual\", \"cell\", \"figure\", \"input\", \"information\"]\n",
      "Any[\"network\", \"networks\", \"neural\", \"training\", \"learning\", \"error\", \"output\", \"layer\", \"input\", \"weights\"]\n",
      "Any[\"gradient\", \"optimization\", \"convergence\", \"convex\", \"algorithm\", \"stochastic\", \"descent\", \"methods\", \"method\", \"rate\"]\n",
      "Any[\"bound\", \"regret\", \"bounds\", \"theorem\", \"algorithm\", \"log\", \"probability\", \"distribution\", \"complexity\", \"lemma\"]\n",
      "Any[\"function\", \"functions\", \"one\", \"set\", \"case\", \"given\", \"optimal\", \"problem\", \"consider\", \"also\"]\n",
      "Any[\"kernel\", \"data\", \"features\", \"learning\", \"feature\", \"methods\", \"space\", \"points\", \"distance\", \"regression\"]\n",
      "Any[\"games\", \"game\", \"player\", \"privacy\", \"players\", \"communication\", \"revenue\", \"strategy\", \"agents\", \"price\"]\n",
      "Any[\"object\", \"attention\", \"image\", \"neural\", \"objects\", \"network\", \"visual\", \"layer\", \"language\", \"convolutional\"]\n",
      "Any[\"data\", \"regression\", \"causal\", \"risk\", \"classifier\", \"classification\", \"selection\", \"using\", \"set\", \"different\"]\n"
     ]
    }
   ],
   "source": [
    "top_N = 10\n",
    "for i in 1:M\n",
    "    #println(\"Topic $i top $top_N words:\")\n",
    "    words, proportions = TopicModels.lda_topicN(i, top_N, corpus, lda);\n",
    "    println(words)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Human in the Loop Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = TopicModels.documentset_readData(\"news-en.txt\")\n",
    "wordPrior = TopicModels.Dirichlet(12, 0.01)\n",
    "M = 3\n",
    "alpha = [0.01 for i in 1:M]\n",
    "topicPrior = TopicModels.Dirichlet(alpha)\n",
    "lda = TopicModels.LDA(topicPrior, wordPrior)\n",
    "TopicModels.lda_sample(corpus.documents, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"Moscow\", \"tank\", \"Russian\", \"unit\", \"Olympic\"]\n",
      "[0.48786, 0.48786, 0.00243, 0.00243, 0.00243]\n",
      "----------------------\n",
      "Any[\"Olympic\", \"medal\", \"Russian\", \"Moscow\", \"Marathon\"]\n",
      "[0.24754, 0.24754, 0.12438, 0.12438, 0.12438]\n",
      "----------------------\n",
      "Any[\"Russian\", \"unit\", \"Self-Defense\", \"Defense\", \"era\"]\n",
      "[0.24754, 0.24754, 0.12438, 0.12438, 0.12438]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for m in 1:3\n",
    "    words, proportions = TopicModels.lda_topicN(m, 5, corpus, lda)\n",
    "    println(words)\n",
    "    println(proportions)\n",
    "    println(\"----------------------\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " Any[2, 1, 2, 3]\n",
       " Any[1, 1, 2, 1]\n",
       " Any[3, 2, 3, 3]\n",
       " Any[2, 1, 1, 1]\n",
       " Any[2, 1, 1, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicModels.lda_removeWord(\"tank\", 1, corpus, lda)\n",
    "TopicModels.lda_gibbsSampling(corpus.documents, 20, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"Moscow\", \"Olympic\", \"medal\", \"runner\", \"era\"]\n",
      "[0.33041, 0.22064, 0.22064, 0.11087, 0.11087]\n",
      "----------------------\n",
      "Any[\"Russian\", \"tank\", \"Marathon\", \"Moscow\", \"unit\"]\n",
      "[0.49183, 0.32843, 0.16503, 0.00163, 0.00163]\n",
      "----------------------\n",
      "Any[\"unit\", \"Self-Defense\", \"Defense\", \"culture\", \"Russian\"]\n",
      "[0.39258, 0.19727, 0.19727, 0.19727, 0.00195]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for m in 1:3\n",
    "    words, proportions = TopicModels.lda_topicN(m, 5, corpus, lda)\n",
    "    println(words)\n",
    "    println(proportions)\n",
    "    println(\"----------------------\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
