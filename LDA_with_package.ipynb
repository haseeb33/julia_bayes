{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Documents/Study/Git_Personal/julia_bayes/TopicModels/Project.toml`\n",
      "┌ Info: Precompiling TopicModels [cfcb1801-bb54-4f1b-8249-336c042d2c46]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "using Distances\n",
    "using StatsBase\n",
    "using DataStructures\n",
    "using ProgressMeter\n",
    "using JSON\n",
    "Pkg.activate(\"TopicModels\")\n",
    "import TopicModels\n",
    "using PyCall\n",
    "using PyPlot\n",
    "\n",
    "using LSHFunctions, LinearAlgebra, BenchmarkTools\n",
    "#corpora = pyimport(\"gensim.corpora\")\n",
    "#ch = pyimport(\"gensim.models.coherencemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'torch' from '/Users/khan/.julia/conda/3/lib/python3.8/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nn = pyimport(\"torch.nn\")\n",
    "np = pyimport(\"numpy\")\n",
    "torch = pyimport(\"torch\")\n",
    "#plt = pyimport(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Run on Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = TopicModels.readData(\"news-en.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordPrior = TopicModels.Dirichlet(12, 0.01)\n",
    "M = 3\n",
    "alpha = [0.01 for i in 1:M];\n",
    "topicPrior = TopicModels.Dirichlet(alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = TopicModels.LDA(topicPrior, wordPrior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = TopicModels.lda_sample(corpus.documents, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1st topic and top 5 words along with vocab proportion\n",
    "words, proportions = TopicModels.lda_topicN(3, 4, corpus, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"medal\", \"runner\", \"era\", \"culture\"]\n",
      "[0.39258, 0.19727, 0.19727, 0.19727]\n"
     ]
    }
   ],
   "source": [
    "println(words)\n",
    "println(proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Human in the Loop Topic Modeling with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, no_lemma_docs = TopicModels.preprocess(\"papers.csv\", 1000); #took 1:35 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lda object from saved json\n",
    "lda = TopicModels.loadLDA(\"lda_obj.json\"); #only takes few seconds \n",
    "\n",
    "#or train new lda model from below command, uncomment to run.\n",
    "#lda = TopicModels.train(corpus, 10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"data\", \"kernel\", \"features\", \"method\", \"approach\", \"methods\", \"point\", \"space\", \"points\", \"learning\"]\n",
      "----------------------\n",
      "Any[\"model\", \"system\", \"figure\", \"neural\", \"network\", \"time\", \"neurons\", \"input\", \"activity\", \"units\"]\n",
      "----------------------\n",
      "Any[\"model\", \"graph\", \"set\", \"algorithm\", \"nodes\", \"node\", \"data\", \"models\", \"time\", \"number\"]\n",
      "----------------------\n",
      "Any[\"learning\", \"state\", \"policy\", \"reward\", \"action\", \"reinforcement\", \"time\", \"function\", \"value\", \"states\"]\n",
      "----------------------\n",
      "Any[\"learning\", \"image\", \"training\", \"images\", \"data\", \"loss\", \"adversarial\", \"model\", \"generative\", \"samples\"]\n",
      "----------------------\n",
      "Any[\"matrix\", \"algorithm\", \"problem\", \"clustering\", \"algorithms\", \"approximation\", \"time\", \"let\", \"pages\", \"one\"]\n",
      "----------------------\n",
      "Any[\"model\", \"models\", \"deep\", \"inference\", \"neural\", \"training\", \"variational\", \"parameters\", \"learning\", \"gradient\"]\n",
      "----------------------\n",
      "Any[\"network\", \"networks\", \"neural\", \"training\", \"input\", \"layer\", \"output\", \"learning\", \"image\", \"set\"]\n",
      "----------------------\n",
      "Any[\"distribution\", \"function\", \"learning\", \"error\", \"probability\", \"data\", \"sample\", \"given\", \"estimation\", \"samples\"]\n",
      "----------------------\n",
      "Any[\"algorithm\", \"optimization\", \"gradient\", \"convergence\", \"convex\", \"algorithms\", \"problem\", \"stochastic\", \"learning\", \"linear\"]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "topics, proportions = TopicModels.show_topics(lda, corpus, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top equal number of docs for each topic(number of docs/number of topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = TopicModels.sortedTopDocsForTopics(lda, corpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[653, 569, 452, 236, 376, 112, 667, 66, 869, 728, 129, 937, 796, 631, 768, 252, 393, 2, 382, 432, 501, 356, 102, 885, 219, 873, 816, 684, 89, 775, 960, 560, 235, 565, 265, 449, 719, 93, 469, 54, 880, 444, 44, 261, 607, 74, 455, 594, 480, 325, 714, 699, 961, 687, 212, 800, 936, 619, 371, 792, 979, 904, 754, 923, 977, 804, 133, 695, 257, 237, 360, 830, 324, 932, 964, 504, 652, 929, 266, 693, 819, 420, 680, 80, 144, 916, 899, 147, 919, 322, 605, 628, 561, 123, 724, 951, 90, 515, 301, 945]\n"
     ]
    }
   ],
   "source": [
    "println(topic_distributions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyphrases are: 1274\n",
      "Unique keyphrases are: 1096\n"
     ]
    }
   ],
   "source": [
    "fileKPDoc = \"kp/simplified_last_1000_keyphrases_clean.json\"\n",
    "fileEMB = \"kp/1000_docs_keyphrase_embedding.json\"\n",
    "fileSim = \"kp/1000_docs_kp_similarity_python.json\"\n",
    "fileKP = \"kp/keyphrases_from_python.json\"\n",
    "kp = TopicModels.load_keyphrase(fileKPDoc, fileEMB, fileSim, fileKP)\n",
    "# Took 0:54 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyphrases are: 1274\n",
      "Unique keyphrases are: 1096\n"
     ]
    }
   ],
   "source": [
    "all_keyphrases_1, documentwise_keyphrases_ls_1 = TopicModels.top_keyphrases_of_topic(kp, topic_distributions, 1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keyphrases_1_ls = [i.first for i in all_keyphrases_1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyphrase_cluster (generic function with 3 methods)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function keyphrase_cluster(kp, topic_doc_kp, sim_threshold=0.85, max_kp_count=5)\n",
    "    kp_topic_ls = [i.first for i in topic_doc_kp];\n",
    "    cluster_kp = []; all_cluster_kps = []; kp_c = 1\n",
    "    for (i_idx, i) in enumerate(kp_topic_ls)\n",
    "        if !(i in all_cluster_kps)\n",
    "            temp_kp_ls = kp.keyphraseSimilarity[i]\n",
    "            temp_dict = Dict(); temp_dict[i] = []\n",
    "            for j in findall(x->x==1, ifelse.(temp_kp_ls.>sim_threshold, true, false))\n",
    "                if kp.keyphrasesOnly[j] in kp_topic_ls\n",
    "                    push!(temp_dict[i], kp.keyphrasesOnly[j])\n",
    "                    push!(all_cluster_kps, kp.keyphrasesOnly[j])\n",
    "                end\n",
    "            end\n",
    "            push!(cluster_kp, temp_dict)\n",
    "            kp_c+=1\n",
    "        end\n",
    "        if kp_c>max_kp_count\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return cluster_kp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kp_1 = keyphrase_cluster(kp, all_keyphrases_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_kp = []\n",
    "all_cluster_kps = []\n",
    "kp_c = 0\n",
    "sim_threshold = 0.85\n",
    "for (idx, i) in enumerate(all_keyphrases_1_ls)\n",
    "    if !(i in all_cluster_kps)\n",
    "        temp_kp_ls = kp.keyphraseSimilarity[i]\n",
    "        temp_dict = Dict()\n",
    "        temp_dict[i] = []\n",
    "        for j in findall(x->x==1, ifelse.(temp_kp_ls.>sim_threshold, true, false))\n",
    "            if kp.keyphrasesOnly[j] in all_keyphrases_1_ls\n",
    "                push!(temp_dict[i], kp.keyphrasesOnly[j])\n",
    "                push!(all_cluster_kps, kp.keyphrasesOnly[j])\n",
    "            end\n",
    "        end\n",
    "        push!(cluster_kp, temp_dict)\n",
    "        kp_c+=1\n",
    "        #print(i)\n",
    "    end\n",
    "    if kp_c>5\n",
    "        break\n",
    "    end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict{Any,Any}(\"gaussian processes\" => Any[\"gaussian processes\", \"gaussian process\", \"stochastic processes\", \"gaussian process regression\", \"nonlinearity\", \"of gaussian processes\", \"mixtures of gaussian distributions\", \"stochastic systems\", \"latent variable multiple output gaussian processes\", \"gaussian mixture model\", \"nondegenerate gaussian processes\"])"
     ]
    }
   ],
   "source": [
    "print(cluster_kp[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict{Any,Any}(\"kernel methods\" => Any[\"kernel methods\", \"functional algorithm\", \"kernel regression\", \"kernel machines\", \"kernel approximation\", \"kernel x free methods\", \"kernel functions\", \"kernel dimension reduction\"])"
     ]
    }
   ],
   "source": [
    "print(cluster_kp_1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_have = []\n",
    "for i in cluster_kp\n",
    "    temp_docs_have = []\n",
    "    for j in collect(values(i))[1]\n",
    "        for (k_idx, k) in enumerate(documentwise_keyphrases_ls_1)\n",
    "            if j in k && !(topic_distributions[1][k_idx] in temp_docs_have)\n",
    "                push!(temp_docs_have, topic_distributions[1][k_idx])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    push!(docs_have, copy(temp_docs_have))\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[102, 129, 219, 252, 356, 360, 371, 376, 607, 631, 667, 680, 695]"
     ]
    }
   ],
   "source": [
    "print(sort(docs_have[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopicModels.Polya(10, TopicModels.Dirichlet(10, [0.6613950891749237, 0.3910087579712004, 0.41621724469367627, 0.3092807128687573, 0.3218985595874629, 0.43918174997401005, 0.35933131008920305, 0.5646429362355259, 0.6561032591272069, 0.43943293662376515], 4.5584925563457315), [265, 10, 439, 0, 3, 212, 73, 6, 1494, 348], 2850)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicPolya[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2683"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.wordPolya[1].n[3180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.apply_refinement(lda, corpus, \"R_D\", docs_have[3], 1); # Take 2:30 mints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopicModels.Polya(10, TopicModels.Dirichlet(10, [1.0e-7, 0.3910087579712004, 0.41621724469367627, 0.3092807128687573, 0.3218985595874629, 0.43918174997401005, 0.35933131008920305, 0.5646429362355259, 0.6561032591272069, 0.43943293662376515], 3.8970975671708077), [0, 217, 106, 37, 645, 173, 176, 124, 1621, 15], 3114)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicPolya[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Any[\"data\", \"kernel\", \"features\", \"method\", \"approach\", \"methods\", \"point\", \"space\", \"points\", \"learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"data\", \"point\", \"approach\", \"method\", \"linear\", \"space\", \"points\", \"used\", \"methods\", \"features\"]\n",
      "----------------------\n",
      "Any[\"model\", \"system\", \"figure\", \"time\", \"neurons\", \"neural\", \"network\", \"input\", \"activity\", \"information\"]\n",
      "----------------------\n",
      "Any[\"model\", \"graph\", \"nodes\", \"set\", \"algorithm\", \"node\", \"data\", \"time\", \"models\", \"tree\"]\n",
      "----------------------\n",
      "Any[\"learning\", \"policy\", \"state\", \"reward\", \"action\", \"reinforcement\", \"time\", \"function\", \"value\", \"states\"]\n",
      "----------------------\n",
      "Any[\"learning\", \"image\", \"training\", \"images\", \"data\", \"model\", \"loss\", \"adversarial\", \"generative\", \"samples\"]\n",
      "----------------------\n",
      "Any[\"matrix\", \"algorithm\", \"clustering\", \"problem\", \"algorithms\", \"approximation\", \"time\", \"random\", \"number\", \"let\"]\n",
      "----------------------\n",
      "Any[\"model\", \"models\", \"deep\", \"neural\", \"inference\", \"learning\", \"training\", \"variational\", \"parameters\", \"data\"]\n",
      "----------------------\n",
      "Any[\"network\", \"networks\", \"neural\", \"training\", \"input\", \"layer\", \"output\", \"learning\", \"set\", \"image\"]\n",
      "----------------------\n",
      "Any[\"distribution\", \"learning\", \"function\", \"error\", \"data\", \"probability\", \"sample\", \"given\", \"samples\", \"estimation\"]\n",
      "----------------------\n",
      "Any[\"algorithm\", \"optimization\", \"gradient\", \"convergence\", \"convex\", \"problem\", \"algorithms\", \"learning\", \"stochastic\", \"function\"]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "TopicModels.show_topics(lda, corpus, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.wordPolya[1].n[3180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[894, 767, 273, 451, 583, 810, 751, 220, 726, 972, 740, 175, 832, 862, 142, 745, 962, 780, 928, 900, 787, 762, 887, 440, 826, 120, 886, 718, 843, 996, 735, 76, 901, 847, 176, 672, 736, 506, 153, 991, 715, 753, 65, 926, 362, 879, 883, 853, 694, 131, 938, 820, 943, 739, 818, 998, 770, 934, 953, 861, 769, 835, 946, 807, 777, 164, 974, 978, 822, 945, 462, 967, 906, 539, 999, 394, 933, 350, 942, 87, 764, 994, 940, 711, 789, 713, 871, 956, 925, 788, 231, 881, 993, 888, 773, 969, 924, 897, 808, 318]\n"
     ]
    }
   ],
   "source": [
    "println(topic_distributions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyphrases are: 1307\n",
      "Unique keyphrases are: 1121\n"
     ]
    }
   ],
   "source": [
    "all_keyphrases_2, documentwise_keyphrases_ls_2 = TopicModels.top_keyphrases_of_topic(kp, topic_distributions, 2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kp_2 = keyphrase_cluster(kp, all_keyphrases_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Base.KeySet{Any,Dict{Any,Any}},1}:\n",
       " [\"neural networks\"]\n",
       " [\"learning\"]\n",
       " [\"associative memory\"]\n",
       " [\"simulation\"]\n",
       " [\"algorithms\"]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[keys(i) for i in cluster_kp_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_have_2 = []\n",
    "for i in cluster_kp_2\n",
    "    temp_docs_have = []\n",
    "    for j in collect(values(i))[1]\n",
    "        for (k_idx, k) in enumerate(documentwise_keyphrases_ls_2)\n",
    "            if j in k && !(topic_distributions[2][k_idx] in temp_docs_have)\n",
    "                push!(temp_docs_have, topic_distributions[2][k_idx])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    push!(docs_have_2, copy(temp_docs_have))\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[65, 76, 131, 142, 153, 164, 175, 220, 318, 394, 451, 462, 672, 711, 713, 736, 739, 751, 764, 767, 770, 777, 787, 808, 832, 847, 871, 881, 888, 897, 901, 925, 933, 934, 940, 953, 956, 969, 974, 991, 993, 998]"
     ]
    }
   ],
   "source": [
    "print(sort(docs_have_2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.apply_refinement(lda, corpus, \"R_D\", docs_have_2[1], 2); # Take 2:30 mints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Any[\"model\", \"system\", \"figure\", \"time\", \"neurons\", \"neural\", \"network\", \"input\", \"activity\", \"information\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"data\", \"point\", \"figure\", \"linear\", \"approach\", \"method\", \"space\", \"points\", \"set\", \"features\"]\n",
      "----------------------\n",
      "Any[\"model\", \"system\", \"time\", \"neural\", \"neurons\", \"information\", \"learning\", \"figure\", \"activity\", \"noise\"]\n",
      "----------------------\n",
      "Any[\"model\", \"graph\", \"nodes\", \"set\", \"algorithm\", \"node\", \"data\", \"time\", \"models\", \"tree\"]\n",
      "----------------------\n",
      "Any[\"learning\", \"state\", \"policy\", \"reward\", \"action\", \"reinforcement\", \"control\", \"time\", \"value\", \"function\"]\n",
      "----------------------\n",
      "Any[\"learning\", \"image\", \"training\", \"images\", \"data\", \"model\", \"loss\", \"adversarial\", \"generative\", \"classification\"]\n",
      "----------------------\n",
      "Any[\"matrix\", \"algorithm\", \"clustering\", \"problem\", \"algorithms\", \"approximation\", \"time\", \"random\", \"pages\", \"number\"]\n",
      "----------------------\n",
      "Any[\"model\", \"models\", \"deep\", \"neural\", \"inference\", \"training\", \"learning\", \"parameters\", \"variational\", \"gradient\"]\n",
      "----------------------\n",
      "Any[\"network\", \"networks\", \"neural\", \"input\", \"training\", \"output\", \"layer\", \"one\", \"learning\", \"units\"]\n",
      "----------------------\n",
      "Any[\"distribution\", \"learning\", \"function\", \"error\", \"probability\", \"data\", \"sample\", \"given\", \"random\", \"regression\"]\n",
      "----------------------\n",
      "Any[\"algorithm\", \"optimization\", \"gradient\", \"convergence\", \"convex\", \"algorithms\", \"problem\", \"learning\", \"stochastic\", \"linear\"]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "TopicModels.show_topics(lda, corpus, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save LDA object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18214193"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_dict = Dict()\n",
    "lda_dict[\"numIteration\"] = lda.numIteration\n",
    "lda_dict[\"M\"] = lda.M\n",
    "lda_dict[\"topicDir_param\"] = lda.topicDir.alpha\n",
    "lda_dict[\"wordPolya_n\"] = [i.n for i in lda.wordPolya]\n",
    "lda_dict[\"X\"] = lda.X\n",
    "lda_dict[\"topicPolya_n\"] = [i.n for i in lda.topicPolya]\n",
    "lda_dict[\"Samples\"] = lda.Samples\n",
    "lda_json_string = JSON.json(lda_dict)\n",
    "\n",
    "open(\"lda_obj.json\",\"w\") do f \n",
    "    write(f, lda_json_string) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda1_raw = JSON.parsefile(\"lda_obj.json\");\n",
    "numIteration = lda1_raw[\"numIteration\"]\n",
    "M = lda1_raw[\"M\"]\n",
    "topicDir = TopicModels.Dirichlet(lda1_raw[\"topicDir_param\"])\n",
    "wordPolya = [TopicModels.Polya(TopicModels.Dirichlet(length(i), 0.01), i) for i in lda1_raw[\"wordPolya_n\"]]\n",
    "X = lda1_raw[\"X\"]\n",
    "topicPolya = [TopicModels.Polya(topicDir, i) for i in lda1_raw[\"topicPolya_n\"]]\n",
    "Samples = lda1_raw[\"Samples\"]\n",
    "lda_obj = TopicModels.LDA(numIteration, M, topicDir, wordPolya, X, topicPolya, Samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Coherence, add and remove word refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7308979585130184"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicModels.topic_coherence(corpus, topics, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.apply_refinement(lda, corpus, \"remove\", \"model\", 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"variable\", \"causal\", \"data\", \"group\", \"test\", \"individual\", \"distribution\", \"information\", \"outcome\", \"hypothesis\"]\n",
      "[0.01333, 0.00915, 0.00838, 0.00817, 0.00716, 0.00677, 0.00609, 0.00606, 0.00596, 0.00581]\n",
      "----------------------\n",
      "Any[\"algorithm\", \"time\", \"problem\", \"one\", \"number\", \"also\", \"result\", \"value\", \"given\", \"two\"]\n",
      "[0.03303, 0.01766, 0.01364, 0.01102, 0.01081, 0.00674, 0.00599, 0.00536, 0.00536, 0.00493]\n",
      "----------------------\n",
      "Any[\"image\", \"network\", \"model\", \"convolutional\", \"training\", \"adversarial\", \"learning\", \"deep\", \"sample\", \"loss\"]\n",
      "[0.02886, 0.017, 0.00936, 0.00924, 0.00892, 0.00814, 0.00783, 0.0075, 0.00735, 0.00707]\n",
      "----------------------\n",
      "Any[\"error\", \"memory\", \"noise\", \"data\", \"distributed\", \"block\", \"bit\", \"vector\", \"performance\", \"code\"]\n",
      "[0.01721, 0.01412, 0.01332, 0.00967, 0.00902, 0.00875, 0.00864, 0.00794, 0.00773, 0.00715]\n",
      "----------------------\n",
      "Any[\"network\", \"unit\", \"neural\", \"input\", \"weight\", \"output\", \"learning\", \"hidden\", \"training\", \"state\"]\n",
      "[0.05848, 0.02447, 0.01927, 0.01882, 0.01732, 0.01724, 0.01314, 0.01215, 0.01033, 0.00933]\n",
      "----------------------\n",
      "Any[\"policy\", \"learning\", \"state\", \"action\", \"reward\", \"agent\", \"reinforcement\", \"arm\", \"task\", \"value\"]\n",
      "[0.03077, 0.02333, 0.02178, 0.01924, 0.01823, 0.01451, 0.01155, 0.00858, 0.00831, 0.00816]\n",
      "----------------------\n",
      "Any[\"regret\", \"algorithm\", \"online\", \"learning\", \"bandit\", \"loss\", \"bound\", \"function\", \"round\", \"player\"]\n",
      "[0.02419, 0.02191, 0.0176, 0.01431, 0.01237, 0.0121, 0.01117, 0.01007, 0.00867, 0.00781]\n",
      "----------------------\n",
      "Any[\"function\", \"point\", \"equation\", \"error\", \"linear\", \"space\", \"local\", \"vector\", \"case\", \"learning\"]\n",
      "[0.02075, 0.01612, 0.01019, 0.00863, 0.00844, 0.0082, 0.0076, 0.00749, 0.00713, 0.00705]\n",
      "----------------------\n",
      "Any[\"bound\", \"theorem\", \"distribution\", \"sample\", \"probability\", \"let\", \"log\", \"result\", \"error\", \"proof\"]\n",
      "[0.0198, 0.01702, 0.01553, 0.01369, 0.01133, 0.01054, 0.01051, 0.0099, 0.00807, 0.00763]\n",
      "----------------------\n",
      "Any[\"neuron\", \"cell\", \"model\", \"response\", \"neural\", \"activity\", \"time\", \"spike\", \"system\", \"signal\"]\n",
      "[0.01951, 0.0149, 0.01434, 0.01009, 0.00916, 0.00914, 0.00815, 0.00726, 0.00714, 0.00669]\n",
      "----------------------\n",
      "Any[\"network\", \"neural\", \"training\", \"layer\", \"deep\", \"learning\", \"gradient\", \"parameter\", \"weight\", \"method\"]\n",
      "[0.04283, 0.02526, 0.0241, 0.02409, 0.02091, 0.02022, 0.01775, 0.00926, 0.0092, 0.0091]\n",
      "----------------------\n",
      "Any[\"model\", \"distribution\", \"inference\", \"data\", \"process\", \"posterior\", \"variational\", \"parameter\", \"variable\", \"bayesian\"]\n",
      "[0.0414, 0.01713, 0.01569, 0.01369, 0.01287, 0.01263, 0.01161, 0.01039, 0.01004, 0.0099]\n",
      "----------------------\n",
      "Any[\"kernel\", \"learning\", \"function\", \"regression\", \"loss\", \"machine\", \"feature\", \"space\", \"method\", \"approximation\"]\n",
      "[0.04227, 0.02353, 0.0213, 0.02076, 0.01437, 0.01309, 0.01084, 0.0102, 0.01019, 0.00892]\n",
      "----------------------\n",
      "Any[\"object\", \"model\", \"image\", \"map\", \"figure\", \"representation\", \"information\", \"visual\", \"user\", \"location\"]\n",
      "[0.01487, 0.01423, 0.01296, 0.01044, 0.01044, 0.01017, 0.00843, 0.00752, 0.00733, 0.00689]\n",
      "----------------------\n",
      "Any[\"graph\", \"node\", \"algorithm\", \"clustering\", \"cluster\", \"edge\", \"function\", \"set\", \"network\", \"model\"]\n",
      "[0.03165, 0.029, 0.01854, 0.01656, 0.01598, 0.0146, 0.01288, 0.01247, 0.00889, 0.00814]\n",
      "----------------------\n",
      "Any[\"model\", \"task\", \"learning\", \"attention\", \"sequence\", \"vector\", \"language\", \"neural\", \"word\", \"question\"]\n",
      "[0.02283, 0.01793, 0.01183, 0.01039, 0.01014, 0.00891, 0.00884, 0.00874, 0.00862, 0.00752]\n",
      "----------------------\n",
      "Any[\"data\", \"set\", \"method\", \"feature\", \"approach\", \"point\", \"performance\", \"datasets\", \"distance\", \"selection\"]\n",
      "[0.03534, 0.01989, 0.01901, 0.01838, 0.0126, 0.00975, 0.00796, 0.00784, 0.00754, 0.00743]\n",
      "----------------------\n",
      "Any[\"matrix\", \"rank\", \"tensor\", \"spectral\", \"method\", \"covariance\", \"sparse\", \"column\", \"signal\", \"decomposition\"]\n",
      "[0.0769, 0.01262, 0.01251, 0.01177, 0.01166, 0.0093, 0.00911, 0.00875, 0.00758, 0.0073]\n",
      "----------------------\n",
      "Any[\"optimization\", \"gradient\", \"method\", \"algorithm\", \"problem\", \"convergence\", \"convex\", \"iteration\", \"function\", \"stochastic\"]\n",
      "[0.02342, 0.02255, 0.02022, 0.01895, 0.01658, 0.01657, 0.01595, 0.01299, 0.01239, 0.01127]\n",
      "----------------------\n",
      "Any[\"model\", \"training\", \"prediction\", \"learning\", \"class\", \"classifier\", \"classification\", \"data\", \"feature\", \"label\"]\n",
      "[0.02777, 0.0205, 0.01664, 0.01537, 0.01535, 0.01453, 0.01351, 0.01289, 0.01285, 0.0128]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "topics, proportions = TopicModels.show_topics(corpus, lda, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6578622641487515"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicModels.topic_coherence(corpus, topics, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.apply_refinement(corpus, lda, \"add\", \"node\", 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.show_topics(corpus, lda, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the top topic for each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function topic_of_each_doc(corpus, lda)\n",
    "    top_topic_for_each_doc = []\n",
    "    for i in 1:corpus.document_size\n",
    "        top_val = 0\n",
    "        top_topic = 0\n",
    "        for j in 1:lda.M\n",
    "            v = TopicModels.lda_topicPredict(i, j, lda)\n",
    "            if v>=top_val\n",
    "                top_val = v\n",
    "                top_topic = j\n",
    "            end\n",
    "        end\n",
    "        push!(top_topic_for_each_doc, top_topic)\n",
    "    end\n",
    "    return top_topic_for_each_doc\n",
    "end\n",
    "top_topic_for_each_doc = topic_of_each_doc(corpus, lda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999-element Array{Any,1}:\n",
       "  2\n",
       "  6\n",
       "  2\n",
       "  1\n",
       "  6\n",
       "  4\n",
       "  9\n",
       "  1\n",
       "  6\n",
       "  5\n",
       "  1\n",
       "  5\n",
       "  5\n",
       "  ⋮\n",
       "  9\n",
       "  6\n",
       " 10\n",
       "  6\n",
       " 10\n",
       " 10\n",
       " 10\n",
       " 10\n",
       "  6\n",
       " 10\n",
       " 10\n",
       "  5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topic_for_each_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "view_top_docs (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function view_top_docs(top_topic_for_each_doc, document_file, topic)\n",
    "    papers = CSV.read(document_file, DataFrame);\n",
    "    titles = papers.title\n",
    "    titles = titles[length(titles)-corpus.document_size+1:length(titles)]\n",
    "    for (idx, t) in enumerate(top_topic_for_each_doc)\n",
    "        if t == topic\n",
    "            #println(idx, \" \" , titles[idx])\n",
    "            print(idx, \", \")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14, 15, 24, 51, 64, 70, 76, 79, "
     ]
    }
   ],
   "source": [
    "using CSV, DataFrames\n",
    "view_top_docs(top_topic_for_each_doc, \"papers.csv\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLTM remove document implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, no_lemma_docs = TopicModels.preprocess(\"papers.csv\", 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = TopicModels.train(corpus, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"network\", \"input\", \"networks\", \"training\", \"learning\"]\n",
      "[0.02076, 0.01477, 0.01446, 0.01322, 0.01085]\n",
      "----------------------\n",
      "Any[\"neural\", \"algorithm\", \"data\", \"case\", \"weight\"]\n",
      "[0.01855, 0.01428, 0.01418, 0.01023, 0.00927]\n",
      "----------------------\n",
      "Any[\"state\", \"model\", \"learning\", \"models\", \"number\"]\n",
      "[0.01429, 0.01354, 0.01193, 0.01107, 0.00774]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "topics, proportions = TopicModels.show_topics(corpus, lda, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{TopicModels.Polya,1}:\n",
       " TopicModels.Polya(5280, TopicModels.Dirichlet(5280, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 52.800000000000004), [7, 36, 8, 10, 14, 128, 17, 7, 0, 0  …  1, 0, 0, 0, 0, 2, 0, 0, 0, 1], 9628)\n",
       " TopicModels.Polya(5280, TopicModels.Dirichlet(5280, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 52.800000000000004), [0, 0, 0, 0, 0, 13, 0, 0, 0, 7  …  0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 9330)\n",
       " TopicModels.Polya(5280, TopicModels.Dirichlet(5280, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 52.800000000000004), [0, 0, 0, 0, 0, 0, 0, 0, 6, 0  …  0, 1, 1, 2, 2, 0, 0, 1, 1, 0], 9252)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.wordPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{TopicModels.Polya,1}:\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [720, 398, 334], 1452)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [111, 370, 1065], 1546)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [1325, 271, 214], 1810)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [963, 138, 561], 1662)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [102, 250, 128], 480)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [559, 274, 422], 1255)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [477, 94, 563], 1134)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [704, 532, 162], 1398)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [106, 693, 242], 1041)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [170, 934, 399], 1503)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [608, 351, 181], 1140)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [544, 1526, 691], 2761)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [143, 131, 592], 866)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [497, 78, 179], 754)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [382, 168, 956], 1506)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [216, 1192, 133], 1541)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [340, 1174, 282], 1796)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [368, 269, 447], 1084)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [948, 137, 536], 1621)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845), [345, 350, 1165], 1860)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopicModels.Dirichlet(3, [0.6906504672532601, 0.6433493639156528, 0.6922273273374719], 2.0262271585063845)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{Any,1}:\n",
       " Any[1, 1, 1, 1, 1, 1, 1, 1, 3, 2  …  1, 1, 3, 1, 1, 3, 3, 3, 1, 2]\n",
       " Any[3, 3, 3, 3, 3, 2, 3, 3, 3, 3  …  2, 2, 3, 3, 2, 3, 2, 3, 3, 3]\n",
       " Any[1, 1, 1, 2, 1, 1, 1, 1, 1, 1  …  3, 1, 1, 1, 1, 1, 1, 1, 3, 1]\n",
       " Any[1, 1, 3, 1, 1, 1, 1, 2, 1, 2  …  3, 3, 1, 1, 2, 1, 3, 1, 1, 3]\n",
       " Any[2, 2, 3, 2, 2, 2, 3, 3, 1, 2  …  1, 1, 3, 3, 1, 2, 3, 3, 2, 1]\n",
       " Any[1, 3, 2, 1, 2, 1, 1, 1, 3, 1  …  2, 3, 1, 3, 3, 2, 3, 3, 2, 3]\n",
       " Any[1, 1, 1, 1, 3, 1, 3, 3, 2, 1  …  3, 1, 3, 1, 3, 1, 1, 1, 3, 3]\n",
       " Any[1, 1, 1, 2, 2, 2, 2, 2, 1, 1  …  1, 2, 2, 2, 1, 1, 2, 1, 2, 1]\n",
       " Any[2, 2, 2, 3, 2, 3, 2, 3, 2, 2  …  2, 3, 1, 2, 3, 1, 1, 3, 2, 2]\n",
       " Any[2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  1, 1, 3, 2, 3, 3, 2, 3, 2, 2]\n",
       " Any[2, 1, 2, 2, 1, 1, 2, 3, 1, 3  …  1, 1, 1, 1, 3, 1, 2, 2, 1, 2]\n",
       " Any[2, 2, 2, 3, 2, 1, 2, 3, 2, 2  …  2, 3, 2, 2, 3, 1, 3, 2, 3, 3]\n",
       " Any[3, 3, 3, 2, 3, 3, 3, 1, 3, 1  …  2, 2, 1, 3, 2, 3, 2, 3, 3, 3]\n",
       " Any[1, 3, 1, 1, 1, 1, 1, 3, 1, 1  …  1, 1, 3, 1, 3, 3, 1, 1, 1, 1]\n",
       " Any[3, 3, 3, 3, 1, 3, 3, 2, 3, 1  …  3, 3, 2, 1, 1, 1, 2, 1, 3, 2]\n",
       " Any[2, 2, 3, 2, 2, 2, 2, 2, 2, 2  …  2, 2, 2, 2, 2, 1, 2, 3, 2, 1]\n",
       " Any[2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  2, 2, 1, 1, 2, 1, 1, 3, 2, 2]\n",
       " Any[3, 1, 3, 3, 2, 3, 1, 1, 3, 3  …  1, 1, 3, 3, 3, 2, 2, 3, 3, 3]\n",
       " Any[1, 1, 1, 1, 1, 2, 3, 3, 1, 3  …  1, 1, 3, 1, 1, 3, 3, 1, 1, 3]\n",
       " Any[3, 3, 3, 3, 3, 3, 2, 3, 3, 2  …  3, 3, 3, 1, 3, 3, 3, 1, 2, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicModels.apply_refinement(lda, corpus, \"R_D\", 1, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{TopicModels.Polya,1}:\n",
       " TopicModels.Polya(5280, TopicModels.Dirichlet(5280, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 52.800000000000004), [0, 0, 0, 0, 0, 39, 0, 0, 0, 0  …  1, 0, 0, 0, 2, 2, 0, 0, 1, 1], 8005)\n",
       " TopicModels.Polya(5280, TopicModels.Dirichlet(5280, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 52.800000000000004), [0, 0, 0, 0, 0, 102, 0, 0, 0, 5  …  0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 9991)\n",
       " TopicModels.Polya(5280, TopicModels.Dirichlet(5280, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01], 52.800000000000004), [7, 36, 8, 10, 14, 0, 17, 7, 6, 2  …  0, 1, 0, 2, 0, 0, 1, 0, 0, 0], 10214)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.wordPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{TopicModels.Polya,1}:\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [1.0e-7, 0.6433493639156528, 0.6922273273374719], 1.3355767912531247), [0, 647, 805], 1452)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [48, 355, 1143], 1546)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [1291, 264, 255], 1810)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [944, 131, 587], 1662)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [78, 235, 167], 480)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [482, 283, 490], 1255)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [480, 84, 570], 1134)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [656, 570, 172], 1398)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [86, 730, 225], 1041)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [120, 1000, 383], 1503)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [619, 344, 177], 1140)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [394, 1715, 652], 2761)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [81, 134, 651], 866)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [497, 88, 169], 754)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [301, 162, 1043], 1506)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [165, 1250, 126], 1541)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [197, 1240, 359], 1796)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [319, 293, 472], 1084)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [962, 137, 522], 1621)\n",
       " TopicModels.Polya(3, TopicModels.Dirichlet(3, [0.4866968873359249, 0.5891369773347052, 0.6502990374700215], 1.7261329021406515), [285, 329, 1246], 1860)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.topicPolya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{Any,1}:\n",
       " Any[3, 3, 3, 3, 3, 2, 3, 3, 3, 2  …  3, 2, 2, 2, 2, 3, 3, 2, 2, 2]\n",
       " Any[3, 2, 3, 3, 3, 3, 3, 3, 3, 3  …  2, 2, 3, 3, 2, 3, 3, 2, 3, 3]\n",
       " Any[1, 1, 1, 2, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 3, 2, 3, 3]\n",
       " Any[2, 1, 1, 1, 1, 3, 1, 1, 1, 3  …  3, 3, 1, 1, 2, 1, 1, 1, 1, 3]\n",
       " Any[2, 2, 1, 2, 2, 2, 3, 2, 3, 2  …  1, 1, 3, 3, 1, 2, 3, 3, 2, 2]\n",
       " Any[1, 3, 2, 1, 2, 3, 3, 1, 3, 3  …  2, 3, 3, 3, 3, 2, 3, 3, 2, 1]\n",
       " Any[1, 1, 1, 1, 3, 1, 3, 3, 2, 3  …  1, 2, 3, 1, 3, 1, 1, 1, 3, 3]\n",
       " Any[1, 1, 2, 2, 1, 2, 2, 2, 1, 1  …  3, 2, 1, 2, 1, 1, 2, 1, 2, 1]\n",
       " Any[2, 2, 2, 2, 2, 1, 2, 3, 2, 2  …  2, 2, 3, 2, 3, 1, 3, 3, 2, 2]\n",
       " Any[2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  1, 2, 3, 2, 3, 3, 2, 1, 2, 2]\n",
       " Any[1, 1, 2, 2, 1, 1, 2, 1, 1, 1  …  2, 1, 1, 2, 3, 1, 2, 2, 1, 1]\n",
       " Any[2, 2, 2, 2, 2, 1, 2, 3, 2, 2  …  2, 3, 2, 2, 3, 2, 3, 2, 3, 2]\n",
       " Any[3, 3, 3, 2, 3, 3, 3, 2, 3, 2  …  2, 2, 3, 3, 2, 3, 2, 3, 3, 3]\n",
       " Any[1, 3, 3, 1, 1, 1, 1, 3, 1, 1  …  1, 1, 3, 1, 1, 3, 1, 1, 1, 1]\n",
       " Any[3, 3, 3, 3, 3, 3, 3, 3, 3, 1  …  3, 3, 2, 3, 1, 1, 3, 3, 3, 1]\n",
       " Any[2, 2, 1, 2, 2, 2, 2, 2, 2, 2  …  2, 1, 2, 2, 2, 2, 1, 3, 2, 1]\n",
       " Any[2, 2, 2, 2, 2, 2, 2, 2, 1, 2  …  2, 2, 1, 1, 2, 3, 2, 3, 2, 3]\n",
       " Any[3, 3, 3, 3, 2, 3, 1, 3, 3, 3  …  1, 3, 3, 3, 1, 2, 1, 3, 3, 3]\n",
       " Any[1, 1, 1, 1, 1, 2, 3, 3, 1, 3  …  2, 1, 3, 1, 3, 1, 3, 1, 3, 3]\n",
       " Any[3, 3, 3, 3, 3, 3, 2, 3, 3, 3  …  3, 3, 3, 1, 3, 3, 1, 1, 2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Topic Labeling Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Documents/Study/Git_Personal/julia_bayes/TopicModels/Project.toml`\n",
      "┌ Info: Precompiling TopicModels [cfcb1801-bb54-4f1b-8249-336c042d2c46]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "using Distances\n",
    "using StatsBase\n",
    "using DataStructures\n",
    "using ProgressMeter\n",
    "Pkg.activate(\"TopicModels\")\n",
    "import TopicModels\n",
    "#using PyCall\n",
    "\n",
    "#corpora = pyimport(\"gensim.corpora\")\n",
    "#ch = pyimport(\"gensim.models.coherencemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, no_lemma_docs = TopicModels.preprocess(\"papers.csv\", 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_label_distribution = TopicModels.train_phrase_model(no_lemma_docs, corpus.vocabulary, 10, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = TopicModels.train(corpus, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, proportions = TopicModels.show_topics(corpus, lda, corpus.vocab_count, false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"input\", \"figure\", \"current\", \"motion\", \"direction\", \"analog\", \"single\", \"voltage\", \"synapses\", \"layer\"]\n",
      "Any[\"state\", \"learning\", \"algorithm\", \"states\", \"probability\", \"function\", \"convergence\", \"value\", \"algorithms\", \"process\"]\n",
      "Any[\"network\", \"neural\", \"units\", \"networks\", \"output\", \"training\", \"control\", \"figure\", \"architecture\", \"model\"]\n",
      "Any[\"model\", \"system\", \"two\", \"field\", \"one\", \"visual\", \"weights\", \"time\", \"first\", \"figure\"]\n",
      "Any[\"recognition\", \"training\", \"distance\", \"input\", \"information\", \"used\", \"pattern\", \"figure\", \"performance\", \"test\"]\n",
      "Any[\"data\", \"algorithm\", \"problem\", \"set\", \"models\", \"new\", \"given\", \"mixture\", \"classification\", \"points\"]\n",
      "Any[\"image\", \"network\", \"networks\", \"mlp\", \"features\", \"road\", \"images\", \"model\", \"objects\", \"layer\"]\n",
      "Any[\"learning\", \"space\", \"generalization\", \"case\", \"training\", \"distribution\", \"examples\", \"sequence\", \"error\", \"matrix\"]\n",
      "Any[\"memory\", \"neurons\", \"learning\", \"computational\", \"model\", \"activation\", \"tasks\", \"neuron\", \"synaptic\", \"different\"]\n",
      "Any[\"neural\", \"error\", \"networks\", \"number\", \"function\", \"network\", \"set\", \"data\", \"results\", \"linear\"]\n"
     ]
    }
   ],
   "source": [
    "for t in topics\n",
    "    println(t[1:10])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mComputing...100%|███████████████████████████████████████| Time: 0:44:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 10 entries:\n",
       "  7  => Array{Any,1}[[\"hidden_layer\", 2.78953, [0.00218779, 0.0116524, 0.010924…\n",
       "  4  => Array{Any,1}[[\"can_be\", 2.56588, [0.00419746, 0.00339036, 0.00328274, 0…\n",
       "  9  => Array{Any,1}[[\"it_is\", 2.80177, [0.00166972, 0.000501499, 0.00517437, 0…\n",
       "  10 => Array{Any,1}[[\"can_be\", 1.9869, [0.000646218, 0.00269087, 0.00172235, 0…\n",
       "  2  => Array{Any,1}[[\"can_be\", 2.29343, [0.00258326, 0.00559643, 0.0037132, 0.…\n",
       "  3  => Array{Any,1}[[\"neural_network\", 2.24291, [0.00704973, 0.00472876, 0.001…\n",
       "  5  => Array{Any,1}[[\"training_data\", 2.66596, [0.00135389, 0.00968897, 0.0006…\n",
       "  8  => Array{Any,1}[[\"can_be\", 2.3162, [0.00559643, 0.00172235, 0.00139951, 0.…\n",
       "  6  => Array{Any,1}[[\"can_be\", 2.27858, [0.00382081, 0.0037132, 0.00349797, 0.…\n",
       "  1  => Array{Any,1}[[\"has_been\", 2.75192, [0.0031105, 0.000865275, 0.00328321,…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New code\n",
    "top_3_bi_tri_labels_only = TopicModels.label_ranking(candidate_label_distribution, topics, proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"has_been\", \"can_be\", \"such_as\"]\n",
      "[\"can_be\", \"value_function\", \"note_that\"]\n",
      "[\"neural_network\", \"error_between\", \"after_training\"]\n",
      "[\"can_be\", \"is_not\", \"has_been\"]\n",
      "[\"training_data\", \"this_is\", \"can_be\"]\n",
      "[\"can_be\", \"it_is\", \"em_algorithm\"]\n",
      "[\"hidden_layer\", \"video_camera\", \"input_features\"]\n",
      "[\"can_be\", \"we_have\", \"it_is\"]\n",
      "[\"it_is\", \"such_as\", \"biological_neural\"]\n",
      "[\"can_be\", \"it_is\", \"this_is\"]\n"
     ]
    }
   ],
   "source": [
    "# For comparing all words of topic distributions with base_count_kl = 0.01\n",
    "for (pos, i) in enumerate(topics)\n",
    "    println([l[1] for l in top_3_bi_tri_labels_only[pos]])\n",
    "    #println(i[1:10]) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For comparing top n words of topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"procedureoptimal\", \"steadily\", \"glennygedacuk\"]\n",
      "Any[\"figure\", \"however\", \"examples\", \"noise\", \"case\", \"point\", \"equation\", \"approximation\", \"rule\", \"given\"]\n",
      "[\"validityinterval\", \"breastcancerwisc\", \"uel\"]\n",
      "Any[\"networks\", \"rules\", \"neural\", \"units\", \"network\", \"mlp\", \"hidden\", \"rule\", \"training\", \"prediction\"]\n",
      "[\"datadriven\", \"irrespective\", \"distort\"]\n",
      "Any[\"image\", \"tasks\", \"object\", \"algorithm\", \"images\", \"one\", \"objects\", \"models\", \"view\", \"learn\"]\n",
      "[\"qlg\", \"mbl\", \"vgo\"]\n",
      "Any[\"wta\", \"input\", \"current\", \"voltage\", \"gate\", \"inversion\", \"source\", \"floating\", \"correlogram\", \"connectivity\"]\n",
      "[\"anatolviical\", \"ity\", \"limonenelinalool\"]\n",
      "Any[\"model\", \"models\", \"activation\", \"inhibitory\", \"output\", \"receptor\", \"layer\", \"excitatory\", \"module\", \"cell\"]\n",
      "[\"jointangle\", \"cameradriven\", \"orderly\"]\n",
      "Any[\"units\", \"memory\", \"unit\", \"patterns\", \"number\", \"pattern\", \"representation\", \"region\", \"joint\", \"active\"]\n",
      "[\"gos\", \"ilx\", \"bkkx\"]\n",
      "Any[\"data\", \"problem\", \"given\", \"one\", \"function\", \"variance\", \"paper\", \"number\", \"mixture\", \"set\"]\n",
      "[\"unreported\", \"bifurcated\", \"contrasting\"]\n",
      "Any[\"cell\", \"analog\", \"figure\", \"chip\", \"neuron\", \"properties\", \"synaptic\", \"neural\", \"new\", \"neurons\"]\n",
      "[\"identification_network\", \"feedback_neural\", \"backpropagating\"]\n",
      "Any[\"network\", \"output\", \"neural\", \"input\", \"networks\", \"figure\", \"control\", \"architecture\", \"feedback\", \"nonlinear\"]\n",
      "[\"metal\", \"monothongal\", \"underfitting\"]\n",
      "Any[\"training\", \"algorithm\", \"set\", \"class\", \"speech\", \"model\", \"neural\", \"vectors\", \"network\", \"classification\"]\n",
      "[\"afscsprojectconnectbench\", \"jackknife\", \"cto\"]\n",
      "Any[\"networks\", \"neural\", \"size\", \"one\", \"results\", \"parameters\", \"functions\", \"since\", \"form\", \"matrix\"]\n",
      "[\"ddo\", \"yasercaltechedu\", \"interprets\"]\n",
      "Any[\"distance\", \"tangent\", \"information\", \"weight\", \"net\", \"used\", \"use\", \"function\", \"transformation\", \"inputs\"]\n",
      "[\"wnt\", \"xvn\", \"xtwv\"]\n",
      "Any[\"error\", \"generalization\", \"learning\", \"space\", \"linear\", \"training\", \"teacher\", \"function\", \"optimal\", \"distribution\"]\n",
      "[\"eyeposition\", \"birds\", \"fanin\"]\n",
      "Any[\"auditory\", \"time\", \"units\", \"weights\", \"network\", \"system\", \"layer\", \"used\", \"two\", \"signal\"]\n",
      "[\"sweeps\", \"perilous\", \"contractionbased\"]\n",
      "Any[\"state\", \"algorithm\", \"states\", \"algorithms\", \"convergence\", \"function\", \"action\", \"policy\", \"markov\", \"value\"]\n",
      "[\"scaleup\", \"maplike\", \"sardines\"]\n",
      "Any[\"input\", \"map\", \"sequence\", \"context\", \"sequences\", \"units\", \"system\", \"feature\", \"representation\", \"word\"]\n",
      "[\"resistant\", \"consolidated\", \"elapsed\"]\n",
      "Any[\"field\", \"learning\", \"visual\", \"motion\", \"motor\", \"model\", \"two\", \"direction\", \"sensory\", \"point\"]\n",
      "[\"bring\", \"meters\", \"retinas\"]\n",
      "Any[\"figure\", \"performance\", \"features\", \"unit\", \"road\", \"information\", \"representations\", \"right\", \"level\", \"new\"]\n",
      "[\"ofreceptor\", \"odorant\", \"spikecoding\"]\n",
      "Any[\"neurons\", \"computational\", \"model\", \"response\", \"simulations\", \"neuron\", \"simulation\", \"number\", \"stem\", \"analysis\"]\n",
      "[\"mccallumccsrochesteredu\", \"inter\", \"suffered\"]\n",
      "Any[\"learning\", \"time\", \"probability\", \"training\", \"machine\", \"error\", \"first\", \"series\", \"order\", \"reinforcement\"]\n"
     ]
    }
   ],
   "source": [
    "# For comparing top 1000 words of topic distributions\n",
    "for (pos, i) in enumerate(topics)\n",
    "    println([l[1] for l in top_3_labels[pos]])\n",
    "    println(i[1:10])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"openversusclosed\", \"procedureoptimal\", \"lksaulopsychemitedu\"]\n",
      "Any[\"figure\", \"however\", \"examples\", \"noise\", \"case\", \"point\", \"equation\", \"approximation\", \"rule\", \"given\"]\n",
      "[\"chowdhury\", \"afscsprojectconnectbench\", \"validityinterval\"]\n",
      "Any[\"networks\", \"rules\", \"neural\", \"units\", \"network\", \"mlp\", \"hidden\", \"rule\", \"training\", \"prediction\"]\n",
      "[\"datadriven\", \"architecuture\", \"neuristique\"]\n",
      "Any[\"image\", \"tasks\", \"object\", \"algorithm\", \"images\", \"one\", \"objects\", \"models\", \"view\", \"learn\"]\n",
      "[\"buccleuch\", \"qlg\", \"mbl\"]\n",
      "Any[\"wta\", \"input\", \"current\", \"voltage\", \"gate\", \"inversion\", \"source\", \"floating\", \"correlogram\", \"connectivity\"]\n",
      "[\"anatolviical\", \"ity\", \"norberto\"]\n",
      "Any[\"model\", \"models\", \"activation\", \"inhibitory\", \"output\", \"receptor\", \"layer\", \"excitatory\", \"module\", \"cell\"]\n",
      "[\"systvol\", \"bartlett\", \"rethworcementlearmng\"]\n",
      "Any[\"units\", \"memory\", \"unit\", \"patterns\", \"number\", \"pattern\", \"representation\", \"region\", \"joint\", \"active\"]\n",
      "[\"nijmegen\", \"psycholinguistik\", \"aonori\"]\n",
      "Any[\"data\", \"problem\", \"given\", \"one\", \"function\", \"variance\", \"paper\", \"number\", \"mixture\", \"set\"]\n",
      "[\"bahram\", \"afscsprojectconnectbench\", \"nonholographic\"]\n",
      "Any[\"cell\", \"analog\", \"figure\", \"chip\", \"neuron\", \"properties\", \"synaptic\", \"neural\", \"new\", \"neurons\"]\n",
      "[\"ujtubt\", \"literaturei\", \"scheduling\"]\n",
      "Any[\"network\", \"output\", \"neural\", \"input\", \"networks\", \"figure\", \"control\", \"architecture\", \"feedback\", \"nonlinear\"]\n",
      "[\"afscsprojectconnectbench\", \"ftpcscmu\", \"psychophysicallymotivated\"]\n",
      "Any[\"training\", \"algorithm\", \"set\", \"class\", \"speech\", \"model\", \"neural\", \"vectors\", \"network\", \"classification\"]\n",
      "[\"afscsprojectconnectbench\", \"cambrdge\", \"rethworcementlearmng\"]\n",
      "Any[\"networks\", \"neural\", \"size\", \"one\", \"results\", \"parameters\", \"functions\", \"since\", \"form\", \"matrix\"]\n",
      "[\"maxmachine\", \"caruanacscmuedu\", \"yasercaltechedu\"]\n",
      "Any[\"distance\", \"tangent\", \"information\", \"weight\", \"net\", \"used\", \"use\", \"function\", \"transformation\", \"inputs\"]\n",
      "[\"wnt\", \"elimination\", \"rethworcementlearmng\"]\n",
      "Any[\"error\", \"generalization\", \"learning\", \"space\", \"linear\", \"training\", \"teacher\", \"function\", \"optimal\", \"distribution\"]\n",
      "[\"openversusclosed\", \"psycholrev\", \"maxmachine\"]\n",
      "Any[\"auditory\", \"time\", \"units\", \"weights\", \"network\", \"system\", \"layer\", \"used\", \"two\", \"signal\"]\n",
      "[\"sweeps\", \"contractionbased\", \"perilous\"]\n",
      "Any[\"state\", \"algorithm\", \"states\", \"algorithms\", \"convergence\", \"function\", \"action\", \"policy\", \"markov\", \"value\"]\n",
      "[\"scaleup\", \"maplike\", \"sardines\"]\n",
      "Any[\"input\", \"map\", \"sequence\", \"context\", \"sequences\", \"units\", \"system\", \"feature\", \"representation\", \"word\"]\n",
      "[\"kaufinann\", \"psychophysicallymotivated\", \"systvol\"]\n",
      "Any[\"field\", \"learning\", \"visual\", \"motion\", \"motor\", \"model\", \"two\", \"direction\", \"sensory\", \"point\"]\n",
      "[\"openversusclosed\", \"afscsprojectconnectbench\", \"rethworcementlearmng\"]\n",
      "Any[\"figure\", \"performance\", \"features\", \"unit\", \"road\", \"information\", \"representations\", \"right\", \"level\", \"new\"]\n",
      "[\"odorant\", \"biochemistry\", \"mathiscscoloradoedu\"]\n",
      "Any[\"neurons\", \"computational\", \"model\", \"response\", \"simulations\", \"neuron\", \"simulation\", \"number\", \"stem\", \"analysis\"]\n",
      "[\"mccallumccsrochesteredu\", \"utomatic\", \"arcidtectures\"]\n",
      "Any[\"learning\", \"time\", \"probability\", \"training\", \"machine\", \"error\", \"first\", \"series\", \"order\", \"reinforcement\"]\n"
     ]
    }
   ],
   "source": [
    "# For comparing top 100 words of topic distributions\n",
    "for (pos, i) in enumerate(topics)\n",
    "    println([l[1] for l in top_3_labels[pos]])\n",
    "    println(i[1:10])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"procedureoptimal\", \"openversusclosed\", \"glennygedacuk\"]\n",
      "Any[\"figure\", \"however\", \"examples\", \"noise\", \"case\", \"point\", \"equation\", \"approximation\", \"rule\", \"given\"]\n",
      "[\"validityinterval\", \"chowdhury\", \"afscsprojectconnectbench\"]\n",
      "Any[\"networks\", \"rules\", \"neural\", \"units\", \"network\", \"mlp\", \"hidden\", \"rule\", \"training\", \"prediction\"]\n",
      "[\"datadriven\", \"architecuture\", \"neuristique\"]\n",
      "Any[\"image\", \"tasks\", \"object\", \"algorithm\", \"images\", \"one\", \"objects\", \"models\", \"view\", \"learn\"]\n",
      "[\"qlg\", \"buccleuch\", \"ioeqo\"]\n",
      "Any[\"wta\", \"input\", \"current\", \"voltage\", \"gate\", \"inversion\", \"source\", \"floating\", \"correlogram\", \"connectivity\"]\n",
      "[\"anatolviical\", \"ity\", \"norberto\"]\n",
      "Any[\"model\", \"models\", \"activation\", \"inhibitory\", \"output\", \"receptor\", \"layer\", \"excitatory\", \"module\", \"cell\"]\n",
      "[\"cameradriven\", \"sloppy\", \"recognizable\"]\n",
      "Any[\"units\", \"memory\", \"unit\", \"patterns\", \"number\", \"pattern\", \"representation\", \"region\", \"joint\", \"active\"]\n",
      "[\"gos\", \"nijmegen\", \"psycholinguistik\"]\n",
      "Any[\"data\", \"problem\", \"given\", \"one\", \"function\", \"variance\", \"paper\", \"number\", \"mixture\", \"set\"]\n",
      "[\"bahram\", \"afscsprojectconnectbench\", \"maxmachine\"]\n",
      "Any[\"cell\", \"analog\", \"figure\", \"chip\", \"neuron\", \"properties\", \"synaptic\", \"neural\", \"new\", \"neurons\"]\n",
      "[\"ujtubt\", \"literaturei\", \"feedback_neural\"]\n",
      "Any[\"network\", \"output\", \"neural\", \"input\", \"networks\", \"figure\", \"control\", \"architecture\", \"feedback\", \"nonlinear\"]\n",
      "[\"afscsprojectconnectbench\", \"underfitting\", \"psychophysicallymotivated\"]\n",
      "Any[\"training\", \"algorithm\", \"set\", \"class\", \"speech\", \"model\", \"neural\", \"vectors\", \"network\", \"classification\"]\n",
      "[\"afscsprojectconnectbench\", \"ftpcscmu\", \"cambrdge\"]\n",
      "Any[\"networks\", \"neural\", \"size\", \"one\", \"results\", \"parameters\", \"functions\", \"since\", \"form\", \"matrix\"]\n",
      "[\"maxmachine\", \"yasercaltechedu\", \"caruanacscmuedu\"]\n",
      "Any[\"distance\", \"tangent\", \"information\", \"weight\", \"net\", \"used\", \"use\", \"function\", \"transformation\", \"inputs\"]\n",
      "[\"wnt\", \"elimination\", \"rethworcementlearmng\"]\n",
      "Any[\"error\", \"generalization\", \"learning\", \"space\", \"linear\", \"training\", \"teacher\", \"function\", \"optimal\", \"distribution\"]\n",
      "[\"openversusclosed\", \"psycholrev\", \"expettation\"]\n",
      "Any[\"auditory\", \"time\", \"units\", \"weights\", \"network\", \"system\", \"layer\", \"used\", \"two\", \"signal\"]\n",
      "[\"sweeps\", \"contractionbased\", \"perilous\"]\n",
      "Any[\"state\", \"algorithm\", \"states\", \"algorithms\", \"convergence\", \"function\", \"action\", \"policy\", \"markov\", \"value\"]\n",
      "[\"maplike\", \"sardines\", \"scaleup\"]\n",
      "Any[\"input\", \"map\", \"sequence\", \"context\", \"sequences\", \"units\", \"system\", \"feature\", \"representation\", \"word\"]\n",
      "[\"kaufinann\", \"resistant\", \"consolidated\"]\n",
      "Any[\"field\", \"learning\", \"visual\", \"motion\", \"motor\", \"model\", \"two\", \"direction\", \"sensory\", \"point\"]\n",
      "[\"openversusclosed\", \"afscsprojectconnectbench\", \"neurallnfonnation\"]\n",
      "Any[\"figure\", \"performance\", \"features\", \"unit\", \"road\", \"information\", \"representations\", \"right\", \"level\", \"new\"]\n",
      "[\"odorant\", \"biochemistry\", \"ofreceptor\"]\n",
      "Any[\"neurons\", \"computational\", \"model\", \"response\", \"simulations\", \"neuron\", \"simulation\", \"number\", \"stem\", \"analysis\"]\n",
      "[\"mccallumccsrochesteredu\", \"utomatic\", \"arcidtectures\"]\n",
      "Any[\"learning\", \"time\", \"probability\", \"training\", \"machine\", \"error\", \"first\", \"series\", \"order\", \"reinforcement\"]\n"
     ]
    }
   ],
   "source": [
    "# For comparing top 50 words of topic distributions\n",
    "for (pos, i) in enumerate(topics)\n",
    "    println([l[1] for l in top_3_labels[pos]])\n",
    "    println(i[1:10])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count in topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278-element Array{Pair{Any,Any},1}:\n",
       "           \"mlp\" => 21\n",
       "          \"risk\" => 15\n",
       "    \"confidence\" => 14\n",
       "     \"bootstrap\" => 12\n",
       "           \"vol\" => 11\n",
       "       \"surgery\" => 9\n",
       "    \"operations\" => 8\n",
       "     \"mortality\" => 8\n",
       "      \"networks\" => 8\n",
       "       \"failure\" => 7\n",
       " \"complications\" => 7\n",
       "       \"history\" => 7\n",
       "         \"renal\" => 7\n",
       "                 ⋮\n",
       "       \"factors\" => 1\n",
       "  \"interactions\" => 1\n",
       "       \"strokes\" => 1\n",
       "     \"committee\" => 1\n",
       "        \"number\" => 1\n",
       "      \"maintain\" => 1\n",
       "         \"block\" => 1\n",
       "       \"reflect\" => 1\n",
       "       \"summary\" => 1\n",
       "       \"average\" => 1\n",
       "      \"spending\" => 1\n",
       "   \"performance\" => 1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort(collect(labels_freq[\"coronary_artery_bypass\"]), by=x->x[2], rev=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 15739 entries:\n",
       "  \"oblique\"     => Dict{Any,Any}(\"points\"=>5,\"jlkl\"=>2,\"translation\"=>2,\"sekule…\n",
       "  \"dev\"         => Dict{Any,Any}(\"fpxsftvg\"=>1,\"fprr\"=>1,\"neurophysiol\"=>1,\"res…\n",
       "  \"cambrdge\"    => Dict{Any,Any}(\"proceedings\"=>1,\"learningfrom\"=>1,\"tesauro\"=>…\n",
       "  \"yjk\"         => Dict{Any,Any}(\"corresponding\"=>1,\"rotation\"=>1,\"points\"=>2,\"…\n",
       "  \"null\"        => Dict{Any,Any}(\"regions\"=>2,\"uext\"=>1,\"oblique\"=>1,\"excitatio…\n",
       "  \"inflowbased\" => Dict{Any,Any}(\"choices\"=>1,\"accommodate\"=>1,\"optimal\"=>1,\"in…\n",
       "  \"ztt\"         => Dict{Any,Any}(\"qlearning\"=>1,\"presented\"=>2,\"directly\"=>1,\"m…\n",
       "  \"iaan\"        => Dict{Any,Any}(\"want\"=>2,\"elog\"=>2,\"emax\"=>2,\"holds\"=>3,\"fini…\n",
       "  \"subfeature\"  => Dict{Any,Any}(\"tasks\"=>1,\"due\"=>1,\"indicate\"=>1,\"relevant\"=>…\n",
       "  \"rises\"       => Dict{Any,Any}(\"constant\"=>3,\"functional\"=>1,\"los\"=>2,\"equal\"…\n",
       "  \"hampshire\"   => Dict{Any,Any}(\"constant\"=>1,\"action\"=>1,\"equal\"=>1,\"data\"=>1…\n",
       "  \"dzfk\"        => Dict{Any,Any}(\"fpxsftvg\"=>1,\"dem\"=>1,\"fprr\"=>1,\"kxt\"=>1,\"fqm…\n",
       "  \"vapnik\"      => Dict{Any,Any}(\"functional\"=>1,\"regions\"=>2,\"parallel\"=>1,\"in…\n",
       "  \"progression\" => Dict{Any,Any}(\"functional\"=>1,\"damage\"=>1,\"gradual\"=>1,\"lesi…\n",
       "  \"neumann\"     => Dict{Any,Any}(\"extraction\"=>1,\"singlelayer\"=>1,\"optimal\"=>1,…\n",
       "  \"fram\"        => Dict{Any,Any}(\"variance\"=>2,\"decisions\"=>2,\"gain\"=>1,\"expert…\n",
       "  \"gathered\"    => Dict{Any,Any}(\"satellite\"=>1,\"approximates\"=>1,\"planetary\"=>…\n",
       "  \"eeitutery\"   => Dict{Any,Any}(\"pre\"=>1,\"pointsets\"=>2,\"objects\"=>3,\"appropri…\n",
       "  \"arborize\"    => Dict{Any,Any}(\"striking\"=>1,\"connections\"=>1,\"selectively\"=>…\n",
       "  \"november\"    => Dict{Any,Any}(\"generalization\"=>1,\"amir\"=>1,\"university\"=>1,…\n",
       "  \"stress\"      => Dict{Any,Any}(\"tasks\"=>5,\"introduced\"=>1,\"optimal\"=>1,\"would…\n",
       "  \"zqm\"         => Dict{Any,Any}(\"aefoc\"=>1,\"vwd\"=>1,\"dmhyrtsmfys\"=>1,\"gxwky\"=>…\n",
       "  \"rectified\"   => Dict{Any,Any}(\"excitation\"=>1,\"nadel\"=>1,\"stable\"=>1,\"square…\n",
       "  \"obey\"        => Dict{Any,Any}(\"whereas\"=>1,\"known\"=>1,\"firstorder\"=>1,\"simpl…\n",
       "  \"methods\"     => Dict{Any,Any}(\"rumeihart\"=>1,\"perturbations\"=>1,\"newly\"=>1,\"…\n",
       "  ⋮             => ⋮"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 14707 entries:\n",
       "  \"oblique\"     => 7247\n",
       "  \"dev\"         => 16\n",
       "  \"cambrdge\"    => 3145\n",
       "  \"yjk\"         => 7246\n",
       "  \"null\"        => 1728\n",
       "  \"inflowbased\" => 1831\n",
       "  \"ztt\"         => 4061\n",
       "  \"iaan\"        => 6949\n",
       "  \"subfeature\"  => 10738\n",
       "  \"rises\"       => 6074\n",
       "  \"hampshire\"   => 11903\n",
       "  \"dzfk\"        => 8\n",
       "  \"vapnik\"      => 3387\n",
       "  \"progression\" => 8512\n",
       "  \"neumann\"     => 5360\n",
       "  \"fram\"        => 10918\n",
       "  \"gathered\"    => 7052\n",
       "  \"eeitutery\"   => 7290\n",
       "  \"arborize\"    => 10152\n",
       "  \"november\"    => 6778\n",
       "  \"stress\"      => 5963\n",
       "  \"zqm\"         => 8291\n",
       "  \"rectified\"   => 6644\n",
       "  \"obey\"        => 2264\n",
       "  \"methods\"     => 818\n",
       "  ⋮             => ⋮"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_output\n",
      "coronary_artery_bypass\n",
      "neural_information_processing_systems\n",
      "i_i_i\n",
      "using_neural_networks\n",
      "as_shown_in_figure\n",
      "m__m\n",
      "can_be_obtained\n",
      "networks_of_spiking_neurons\n",
      "we_assume_that\n",
      "i__i_\n",
      "this_paper_we\n",
      "this_can_be\n",
      "can_be_derived\n",
      "learning_from_examples\n",
      "it_is_possible\n",
      "editors_advances_in_neural_information\n",
      "i_i_i_i\n",
      "cambridge_ma\n",
      "office_of_naval_research\n",
      "city_block_length\n",
      "can_be_viewed_as\n",
      "radial_basis_function\n",
      "where_n\n",
      "teacher_space_entropy\n",
      "we_then\n",
      "output_is\n",
      "is_defined_as\n",
      "that_can_be\n",
      "we_find_that\n",
      "et_al_eds\n",
      "san_mateo_ca\n",
      "can_be_used\n",
      "modified_actorcritic_algorithm\n",
      "hierarchical_mixtures_of_experts\n",
      "which_can_be\n",
      "training_algorithms\n",
      "it_is_not\n",
      "et_al\n",
      "all_other\n",
      "processing_systems_san_mateo\n",
      "network_is\n",
      "processing_systems_morgan_kaufmann\n",
      "will_not_be\n",
      "m_is\n",
      "pittsburgh_pa\n",
      "where_is\n",
      "elastic_input_field\n",
      "eds_advances_in_neural_information\n",
      "it_can_be_shown\n",
      "l_d\n",
      "it_does_not\n",
      "can_be_found\n",
      "here_is\n",
      "carnegie_mellon_university\n",
      "way_that\n",
      "it_may_be\n",
      "information_processing_systems\n",
      "terrence_j_sejnowski\n",
      "there_is_no\n",
      "receiver_operating_characteristic\n",
      "this_paper_is\n",
      "hidden_markov_models\n",
      "i__i\n",
      "artificial_neural_networks\n",
      "be_shown_that\n",
      "we_first\n",
      "can_be_done\n",
      "chaotic_time_series\n",
      "can_be_interpreted_as\n",
      "time_series_prediction\n",
      "can_be_seen\n",
      "abstract_this_paper\n",
      "this_is_not\n",
      "advances_in_neural_information_processing\n",
      "san_mateo_ca_morgan\n",
      "this_paper_presents\n",
      "c__c_\n",
      "we_see_that\n",
      "department_of_computer_science\n",
      "principal_component_analysis\n",
      "single_transistor\n",
      "is_shown_in_figure\n",
      "have_the_same\n",
      "morgan_kaufmann_publishers\n",
      "neural_network_control\n",
      "soft_state_aggregation\n",
      "there_are_two\n",
      "probability_one\n",
      "michael_i_jordan\n",
      "as_long_as\n",
      "ca_morgan_kaufmann\n",
      "instancebased_state_identification\n",
      "we_show_that\n",
      "x_is\n",
      "daniel_m_wolpert\n",
      "have_shown_that\n",
      "is_that\n",
      "can_be_achieved\n",
      "it_has_been\n",
      "as_well_as\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for (phrase, score) in phrase_model_bigram.find_phrases(bigrams_docs)\n",
    "    println(phrase)\n",
    "    s+=1\n",
    "end\n",
    "println(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech related random commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17802-element Array{Tuple{String,String},1}:\n",
       " (\"A\", \"DT\")\n",
       " (\"n\", \"JJ\")\n",
       " (\" \", \"NN\")\n",
       " (\"A\", \"NNP\")\n",
       " (\"l\", \"NN\")\n",
       " (\"t\", \"NN\")\n",
       " (\"e\", \"NN\")\n",
       " (\"r\", \"NN\")\n",
       " (\"n\", \"IN\")\n",
       " (\"a\", \"DT\")\n",
       " (\"t\", \"NN\")\n",
       " (\"i\", \"NN\")\n",
       " (\"v\", \"VBP\")\n",
       " ⋮\n",
       " (\"p\", \"JJ\")\n",
       " (\"u\", \"JJ\")\n",
       " (\"t\", \"NN\")\n",
       " (\"a\", \"DT\")\n",
       " (\"t\", \"NN\")\n",
       " (\"i\", \"NN\")\n",
       " (\"o\", \"VBP\")\n",
       " (\"n\", \"NN\")\n",
       " (\".\", \".\")\n",
       " (\"\\n\", \"CC\")\n",
       " (\"\\n\", \"JJ\")\n",
       " (\"\\f\", \"NN\")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= nltk.pos_tag(docs[7143])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: a not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: a not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at ./In[3]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [3] execute_code(::String, ::String) at /Users/khan/.julia/packages/IJulia/a1SNk/src/execute_request.jl:27",
      " [4] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /Users/khan/.julia/packages/IJulia/a1SNk/src/execute_request.jl:86",
      " [5] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [6] invokelatest at ./essentials.jl:709 [inlined]",
      " [7] eventloop(::ZMQ.Socket) at /Users/khan/.julia/packages/IJulia/a1SNk/src/eventloop.jl:8",
      " [8] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "for i in a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'nltk' from '/Users/khan/.julia/conda/3/lib/python3.8/site-packages/nltk/__init__.py'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "nltk = pyimport(\"nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <WordNetLemmatizer>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Word\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lemmatize(\"Word\", pos=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: nltk not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: nltk not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[8]:2",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [3] execute_code(::String, ::String) at /Users/khan/.julia/packages/IJulia/a1SNk/src/execute_request.jl:27",
      " [4] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /Users/khan/.julia/packages/IJulia/a1SNk/src/execute_request.jl:86",
      " [5] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [6] invokelatest at ./essentials.jl:709 [inlined]",
      " [7] eventloop(::ZMQ.Socket) at /Users/khan/.julia/packages/IJulia/a1SNk/src/eventloop.jl:8",
      " [8] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "using DataStructures\n",
    "wn = nltk.corpus.wordnet\n",
    "t_map = Dict('J' => wn.ADJ,'V' => wn.VERB,'R' => wn.ADV)\n",
    "\n",
    "tag_map = DefaultDict(wn.NOUN, t_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map['Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Char,Int64} with 2 entries:\n",
       "  'a' => 1\n",
       "  'b' => 2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = DefaultDict(1) \n",
    "d = Dict('a'=>1, 'b'=>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultDict{Char,Int64,Int64} with 2 entries:\n",
       "  'a' => 1\n",
       "  'b' => 2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = DefaultDict(0, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['A']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
